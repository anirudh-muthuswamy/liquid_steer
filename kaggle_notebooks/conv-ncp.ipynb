{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11382018,"sourceType":"datasetVersion","datasetId":7126841}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ncps\n!pip install torchsummary\n!pip install torchinfo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T19:42:28.226768Z","iopub.execute_input":"2025-04-20T19:42:28.227026Z","iopub.status.idle":"2025-04-20T19:42:37.156975Z","shell.execute_reply.started":"2025-04-20T19:42:28.227003Z","shell.execute_reply":"2025-04-20T19:42:37.156052Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: ncps in /usr/local/lib/python3.11/dist-packages (1.0.1)\nRequirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ncps) (1.0.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ncps) (24.2)\nRequirement already satisfied: torchsummary in /usr/local/lib/python3.11/dist-packages (1.5.1)\nRequirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport cv2\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport os\nimport torch\nimport numpy as np\nimport cv2\nfrom collections import OrderedDict\nfrom itertools import islice\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport torch.nn as nn\nfrom ncps.wirings import NCP\nfrom ncps.torch import LTC\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nfrom tqdm import tqdm\nfrom torchinfo import summary\nimport scipy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T19:42:37.158564Z","iopub.execute_input":"2025-04-20T19:42:37.158836Z","iopub.status.idle":"2025-04-20T19:42:44.119883Z","shell.execute_reply.started":"2025-04-20T19:42:37.158810Z","shell.execute_reply":"2025-04-20T19:42:44.119315Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Check Data","metadata":{}},{"cell_type":"code","source":"pd.set_option(\"display.max_rows\", 200)\nplt.ion() \n\ndef get_full_image_filepaths(data_dir):\n    filepaths = os.listdir(data_dir)\n    sorted_filepaths = sorted(filepaths, key=lambda x: int(re.search(r'\\d+', x).group()))\n    full_filepaths = [os.path.join(data_dir, file) for file in sorted_filepaths]\n\n    return full_filepaths\n\ndef get_steering_angles(path):\n    file = open(path, 'r')\n    lines = file.readlines()\n    #change this to line[:-1] if using kaggle dataset else keep the same for sullychen dataset\n    steering_angles = [(line[:-1]).split(' ')[1].split(',')[0] for line in lines]\n    timestamps = [line.strip().split(' ', 1)[1].split(',')[1] for line in lines]\n\n    return [steering_angles, timestamps]\n\ndef convert_to_df(full_filepaths, steering_angles, timestamps, norm=True):\n    data = pd.DataFrame({'filepath':full_filepaths,'steering_angle':steering_angles, 'timestamps':timestamps})\n    data['parsed_timestamp'] = data['timestamps'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S:%f'))\n    data = data.drop('timestamps', axis=1)\n    data = data.reset_index(drop=True)\n    data['steering_angle'] = data['steering_angle'].astype('float')\n    #normalize steering angles to -1,1 based on actual steering range (-360, 360) of wheel\n    if norm == True:\n        data['steering_angle'] = data['steering_angle']  / 360\n    return data\n\ndef display_images_with_angles(data_pd, steering_wheel_img):\n\n    # steering wheel image\n    img = cv2.imread(steering_wheel_img,0)\n    rows,cols = img.shape\n    i = 0\n    xs = data_pd['filepath'].values\n    ys = data_pd['steering_angle'].values\n\n    #Press q to stop \n    while(cv2.waitKey(100) != ord('q')) and i < len(xs):\n        try:\n            # Driving Test Image displayed as Video\n            full_image = cv2.imread(xs[i])\n\n            degrees = ys[i] * 360\n            print(\"Steering angle: \" + str(degrees) + \" (actual)\")\n            cv2.imshow(\"frame\", full_image)\n\n            # Angle at which the steering wheel image should be rotated\n            M = cv2.getRotationMatrix2D((cols/2,rows/2),-degrees,1)\n            dst = cv2.warpAffine(img,M,(cols,rows))\n\n            cv2.imshow(\"steering wheel\", dst)\n        except:\n            print('ERROR at', i)\n        i += 1\n    cv2.destroyAllWindows()\n\ndef disp_freq_steering_angles(data_pd):\n    # Define bin edges from -1 to 1 with step 0.1\n    bin_edges = list(range(-10, 11))  # Since steering angle is between -1 and 1, multiply by 10\n    bin_edges = [x / 10 for x in bin_edges]  # Convert back to decimal values\n\n    # Assign steering angles to bins\n    data_pd['binned'] = pd.cut(data_pd['steering_angle'].astype('float'), bins=bin_edges, right=False)\n\n    # Count occurrences in each bin\n    bin_counts = data_pd['binned'].value_counts().sort_index()\n\n    # Plot bar chart\n    plt.figure(figsize=(12, 5))\n    bin_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n\n    # Formatting\n    plt.xlabel(\"Steering Angle Range\")\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Frequency of Steering Angles in 0.1 Intervals\")\n    plt.xticks(rotation=45)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n\n    plt.show()\n\ndef disp_start_and_end_in_filtered_data(data_pd_filtered):\n    turn_starts = data_pd_filtered[data_pd_filtered['turn_shift'] == 1]\n    turn_ends = data_pd_filtered[data_pd_filtered['turn_shift'] == -1]\n\n    plt.figure(figsize=(12, 5))\n    plt.plot(data_pd_filtered.index, data_pd_filtered['steering_angle'], label=\"Steering Angle\", alpha=0.5)\n    plt.scatter(turn_starts.index, turn_starts['steering_angle'], color='red', label=\"Turn Start\", marker=\"o\")\n    plt.scatter(turn_ends.index, turn_ends['steering_angle'], color='blue', label=\"Turn End\", marker=\"x\")\n\n    plt.xlabel(\"Frame Index\")\n    plt.ylabel(\"Steering Angle\")\n    plt.title(\"Turn Start and End Points in Steering Angle Data\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef filter_df_on_turns(data_pd, turn_threshold = 0.06, buffer_before = 60, buffer_after = 60):\n    # Parameters\n    turn_threshold = turn_threshold  # Define turn threshold (absolute value)\n    buffer_before = buffer_before    # Frames to include before a turn\n    buffer_after = buffer_after     # Frames to include after a turn\n\n    # Load your dataset (assuming it's a DataFrame named df)\n    data_pd['index'] = data_pd.index  # Preserve original ordering if needed\n\n    # Identify where turning happens\n    data_pd['turning'] = (data_pd['steering_angle'].abs() > turn_threshold).astype(int)\n\n    # Find where turns start and end\n    data_pd['turn_shift'] = data_pd['turning'].diff()  # 1 indicates start, -1 indicates end\n\n    # Get turn start and end indices\n    turn_starts = data_pd[data_pd['turn_shift'] == 1].index\n    turn_ends = data_pd[data_pd['turn_shift'] == -1].index\n\n    # Ensure equal number of start and end points\n    if len(turn_ends) > 0 and turn_starts[0] > turn_ends[0]:  \n        turn_ends = turn_ends[1:]  # Drop the first turn_end if it comes before a start\n\n    # Selected indices for keeping\n    selected_indices = set()\n\n    for start, end in zip(turn_starts, turn_ends):\n        # Include a buffer of frames before and after\n        start_idx = max(0, start - buffer_before)\n        end_idx = min(len(data_pd) - 1, end + buffer_after)\n        \n        # Add indices to selection\n        selected_indices.update(range(start_idx, end_idx + 1))\n\n    # Create filtered dataset\n    data_pd_filtered = data_pd.loc[sorted(selected_indices)].reset_index(drop=True)\n\n    # Drop temporary columns\n    data_pd_filtered = data_pd_filtered.drop(columns=['turning', 'turn_shift'])\n\n    # Detect sequence breaks (where the original index is not continuous)\n    data_pd_filtered[\"sequence_id\"] = (data_pd_filtered[\"index\"].diff() != 1).cumsum()\n\n    return data_pd_filtered\n\ndef group_data_by_sequences(data_pd_filtered):\n    sequence_lengths = data_pd_filtered.groupby(\"sequence_id\").size()\n\n    print(f\"Minimum sequence length: {min(sequence_lengths)}\")\n    print(f\"Maximum sequence length: {max(sequence_lengths)}\")\n\n    # plt.plot(sequence_lengths)\n\n    # Keep only sequences with at least 10 frames (adjust as needed)\n    valid_sequences = sequence_lengths[sequence_lengths >= 40].index\n    data_pd_filtered = data_pd_filtered[data_pd_filtered[\"sequence_id\"].isin(valid_sequences)]\n\n    print(f\"Total valid sequences: {len(valid_sequences)}\")\n\n    return data_pd_filtered\n\ndef get_preprocessed_data_pd(data_dir, steering_angles_txt_path, filter = True,\n                             turn_threshold = 0.06, buffer_before = 60, buffer_after = 60,\n                             norm=True, save_dir = 'data/csv_files'):\n    img_paths = get_full_image_filepaths(data_dir)\n    steering_angles, timestamps = get_steering_angles(steering_angles_txt_path)\n\n    data_pd = convert_to_df(img_paths, steering_angles, timestamps, norm)\n    if filter and norm:\n        data_pd_filtered = filter_df_on_turns(data_pd, turn_threshold = turn_threshold, \n                                            buffer_before = buffer_before, buffer_after = buffer_after)\n        data_pd_filtered = group_data_by_sequences(data_pd_filtered)\n\n        # Save\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        data_pd_filtered.to_csv(os.path.join(save_dir,f\"flt_ncp_tt_{turn_threshold}_bb_{buffer_before}_ba_{buffer_after}.csv\"), index=False)\n\n        return data_pd_filtered\n    \n    elif filter and not norm:\n        print('Error: If filtering, then steering values should be normalized (-1, 1), set norm to True')\n        exit(1)\n\n    else:\n        sequence_id = 0\n        sequence_ids = [sequence_id]\n\n        # Iterate through rows to calculate time differences (if greater than 3 seconds \n        # or not) and assign sequence IDs\n        for i in range(1, len(data_pd)):\n            time_diff = (data_pd['parsed_timestamp'][i] - data_pd['parsed_timestamp'][i-1]).total_seconds()\n            if time_diff > 3:\n                sequence_id += 1\n            sequence_ids.append(sequence_id)\n\n        # Add sequence_id column to DataFrame\n        data_pd['sequence_id'] = sequence_ids\n\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        data_pd.to_csv(os.path.join(save_dir,f\"ncp_tt_{turn_threshold}_bb_{buffer_before}_ba_{buffer_after}.csv\"), index=False)\n        \n        return data_pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T21:06:57.723309Z","iopub.execute_input":"2025-04-20T21:06:57.723663Z","iopub.status.idle":"2025-04-20T21:06:57.746763Z","shell.execute_reply.started":"2025-04-20T21:06:57.723639Z","shell.execute_reply":"2025-04-20T21:06:57.745935Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"class WeightedMSE(nn.Module):\n    def __init__(self, alpha=0.1):\n        super(WeightedMSE, self).__init__()\n        self.alpha = alpha\n        \n    def forward(self, predictions, targets):\n        # squared error\n        squared_error = (predictions - targets)**2\n        \n        # weighting factor: w(y) = exp(|y|*alpha)\n        weights = torch.exp(torch.abs(targets) * self.alpha)\n        weighted_loss = squared_error * weights\n\n        return weighted_loss.mean()\n\nclass convolutional_head(nn.Module):\n    def __init__(self, num_filters = 8, features_per_filter = 16):\n        super(convolutional_head, self).__init__()\n\n        self.num_filters = num_filters\n        self.features_per_filter = features_per_filter\n\n        self.conv1 = nn.Conv2d(3, 24, kernel_size=5, stride=2, padding=2)\n        self.conv2 = nn.Conv2d(24, 36, kernel_size=5, stride=2, padding=2)\n        self.conv3 = nn.Conv2d(36, 48, kernel_size=3, stride=2, padding=1)\n        self.conv4 = nn.Conv2d(48, 64, kernel_size=3, stride=1, padding=1)\n        self.conv5 = nn.Conv2d(64, num_filters, kernel_size=3, stride=1, padding=1)\n\n        self.relu = nn.ReLU()\n\n        # FC to extract features per filter\n        self.fc_layers = nn.ModuleList([\n            nn.Linear(28 * 28, features_per_filter) for _ in range(num_filters)\n        ])\n\n        self.activations = []\n        self.feature_layer = None\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the convolutional head.\n\n        :param x: Input tensor of shape [batch, channels, height, width]\n        :return: Feature vector of shape [batch, num_filters * features_per_filter]\n        \"\"\"\n\n        self.activations = []\n        batch_size = x.shape[0]\n\n        # apply conv layers\n        # [batch, num_filters, height, width] -> [512, 8, 28, 28] for seq len of 64 and batch size of 8\n\n        x = self.relu(self.conv1(x)); self.activations.append(x)\n        x = self.relu(self.conv2(x)); self.activations.append(x)\n        x = self.relu(self.conv3(x)); self.activations.append(x)\n        x = self.relu(self.conv4(x)); self.activations.append(x)\n        x = self.relu(self.conv5(x)); self.activations.append(x)\n\n        # individual filter outputs\n        filter_outputs = torch.split(x, 1, dim=1)  # splitting along channel dimension -> len of 8 for num_filters = 8\n        #shape of a single filter_output -> [512, 1, 28, 28]\n\n        feature_vectors = []\n        for i in range(self.num_filters):\n            filter_out = filter_outputs[i].view(batch_size, -1)  # flatten each filter output -> shape of: [512, 784]\n            feature_vec = F.relu(self.fc_layers[i](filter_out))  # apply FC layer -> shape of: [512, 4]\n            feature_vectors.append(feature_vec)\n\n        # concat feature vectors\n        feature_layer = torch.cat(feature_vectors, dim=1)  # [batch, num_filters * features_per_filter]\n        # [512, 32]\n        self.feature_layer = feature_layer\n\n        return feature_layer\n    \n    def visual_backprop(self, idx=0):\n        \"\"\"\n        VisualBackprop-like mask computation using torch (GPU compatible).\n        Returns: [H, W] attention mask (still returned as a CPU numpy array).\n        \"\"\"\n        # mean maps for each layer\n        means = []\n\n        for layer_act in self.activations:\n            # [B, C, H, W] -> one sample\n            a = layer_act[idx]  # [C, H, W]\n            a = a.float()\n            per_channel_max = torch.amax(torch.amax(a, dim=1), dim=1) + 1e-6  # [C]\n            norm = a / per_channel_max[:, None, None]  # [C, H, W]\n            mean_map = norm.mean(dim=0)  # [H, W]\n            means.append(mean_map)\n\n        # feature-level activation mask\n        feat_layer = self.feature_layer[idx]  # [num_filters * features_per_filter]\n        feat_layer = torch.abs(feat_layer).view(self.num_filters, self.features_per_filter)  # [F, P]\n        feat_mask = feat_layer.mean(dim=1)  # [F]\n        feat_mask = feat_mask / (feat_mask.max() + 1e-6)\n\n        # applies a rough weighting on last activation map\n        mask = means[-1] * feat_mask.mean()  # [H, W]\n\n        # backward pass through mean activations (resize each to next layer size)\n        for i in range(len(means) - 2, -1, -1):\n            larger = means[i]  # [H, W]\n            smaller = F.interpolate(mask.unsqueeze(0).unsqueeze(0), size=larger.shape, mode='bilinear', align_corners=False)\n            smaller = smaller.squeeze()\n            mask = larger * smaller\n\n        # normalize and move to cpu\n        mask = mask - mask.min()\n        mask = mask / (mask.max() + 1e-6)\n        return mask.detach().cpu().numpy()\n\n    \nclass ConvNCPModel(nn.Module):\n    def __init__(self, num_filters=8, features_per_filter=4, \n                 inter_neurons = 12, command_neurons = 6, motor_neurons = 1, \n                 sensory_fanout = 6, inter_fanout = 4, recurrent_command_synapses = 6,\n                 motor_fanin = 6, seed = 20190120):\n        super(ConvNCPModel, self).__init__()\n\n        # Define NCP wiring based on CommandLayerWormnetArchitecture parameters (from NCP Paper)\n        wiring = NCP(\n            inter_neurons=inter_neurons,   # Number of interneurons\n            command_neurons=command_neurons,  # Number of command neurons\n            motor_neurons=motor_neurons,    # Output neurons (1 for steering)\n            sensory_fanout=sensory_fanout,   # Number of interneurons each sensory neuron connects to\n            inter_fanout=inter_fanout,     # Number of command neurons each interneuron connects to\n            recurrent_command_synapses=recurrent_command_synapses,  # Recurrent connections in the command layer\n            motor_fanin=motor_fanin,      # Number of command neurons each motor neuron connects to\n            seed=seed       # Random seed for reproducibility\n        )\n\n        self.conv_head = convolutional_head(num_filters, features_per_filter)\n\n        self.ltc = LTC(\n            input_size=num_filters * features_per_filter,  # should match the conv head output\n            units=wiring,\n            return_sequences=True\n        )\n\n        # FC layer to map motor neuron output to a steering angle\n        self.fc_out = nn.Linear(wiring.output_dim, 1)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass: Conv Head → LTC-NCP → Fully Connected.\n        :param x: Input shape [batch, seq_len, channels, height, width]\n        :return: Steering angles [batch, seq_len]\n        \"\"\"\n        batch_size, seq_len, c, h, w = x.size()\n\n        # flatten batch and sequence for cnn processing\n        x = x.view(batch_size * seq_len, c, h, w)\n\n        # extract features using conv head\n        features = self.conv_head(x)  # [batch * seq_len, feature_dim]\n\n        # back to [batch, seq_len, feature_dim] for LTC\n        features = features.view(batch_size, seq_len, -1)\n\n        # forward pass through LTC\n        outputs, _ = self.ltc(features) # [batch, seq_len, 1]\n\n        # map NCP output to steering angle\n        predictions = self.fc_out(outputs) # [batch, seq_len, 1]\n        return predictions.squeeze(-1)  # [batch, seq_len]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T19:42:59.297025Z","iopub.execute_input":"2025-04-20T19:42:59.297859Z","iopub.status.idle":"2025-04-20T19:42:59.325204Z","shell.execute_reply.started":"2025-04-20T19:42:59.297825Z","shell.execute_reply":"2025-04-20T19:42:59.324359Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Dataset","metadata":{}},{"cell_type":"code","source":"def df_split_train_val(df_filtered, train_csv_filename, val_csv_filename,\n                       save_dir='data/csv_files',train_size = 0.8):\n    train_dataset = df_filtered[:int(train_size * len(df_filtered))]\n    val_dataset = df_filtered[int(train_size * len(df_filtered)):]\n    print('Train dataset length:', len(train_dataset))\n    print('Val dataset length:', len(val_dataset))\n    print(save_dir)\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    train_dataset.to_csv(os.path.join(save_dir,train_csv_filename), index=False)\n    val_dataset.to_csv(os.path.join(save_dir,val_csv_filename), index=False)\n\n    return os.path.join(save_dir,train_csv_filename), os.path.join(save_dir,val_csv_filename)\n\ndef calculate_mean_and_std(dataset_path):\n    num_pixels = 0\n    channel_sum = np.zeros(3)  # Assuming RGB images, change to (1,) for grayscale\n    channel_sum_squared = np.zeros(3)  # Assuming RGB images, change to (1,) for grayscale\n\n    for root, _, files in os.walk(dataset_path):\n        for file in files:\n            image_path = os.path.join(root, file)\n            image = Image.open(image_path).convert('RGB')  # Convert to RGB if needed\n\n            pixels = np.array(image) / 255.0  # Normalize pixel values between 0 and 1\n            num_pixels += pixels.size // 3  # Assuming RGB images, change to 1 for grayscale\n\n            channel_sum += np.sum(pixels, axis=(0, 1))\n            channel_sum_squared += np.sum(pixels ** 2, axis=(0, 1))\n\n    mean = channel_sum / num_pixels\n    std = np.sqrt((channel_sum_squared / num_pixels) - mean ** 2)\n    return mean, std\n\nclass CustomDataset(Dataset):\n    def __init__(self, csv_file, seq_len, imgh=224, imgw=224, step_size=1, crop=True, transform=None):\n        \"\"\"\n        Dataset that extracts continuous sequences from the given DataFrame.\n\n        :param df: Filtered DataFrame with columns ['filepath', 'steering_angle', 'sequence_id']\n        :param seq_len: Length of each sequence\n        :param transform: Transformations for image preprocessing\n        \"\"\"\n        self.df = pd.read_csv(csv_file)\n        self.seq_len = seq_len\n        self.transform = transform\n        self.imgh = imgh\n        self.imgw = imgw\n        self.step_size = step_size\n        self.crop = crop\n\n        # Group by sequence_id and collect valid sequences\n        self.sequences = OrderedDict({})\n        num_sequences_total = 0\n        # for each sequence id\n        for seq_id in self.df[\"sequence_id\"].unique():\n            seq_data = self.df[self.df[\"sequence_id\"] == seq_id]\n            num_sequences = max((len(seq_data) - self.seq_len)//self.step_size + 1, 0)\n            num_sequences_total += num_sequences\n            # for each sequence of len=self.seq_len for that sequence_id\n            for i in range(0,len(seq_data) - self.seq_len + 1, self.step_size):  # Only full sequences\n                self.sequences[(seq_id,i)] = (seq_data.iloc[i : i + self.seq_len])\n\n        self.index_map = {key: i for i, key in enumerate(self.sequences.keys())}\n\n        print(f\"Total sequences extracted: {len(self.sequences)} using step_size={self.step_size} and seq_len={self.seq_len}\")\n\n    def get_ith_element(self, od, i):\n        return next(islice(od.items(), i, None))\n    \n    def _crop_lower_half(self,img, keep_ratio=0.6):\n        #Crops the bottom `keep_ratio` portion of the image.\n        h = img.shape[0]\n        crop_start = int(h * (1 - keep_ratio))\n        return img[crop_start:, :, :]\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        seq_batch = self.get_ith_element(self.sequences, idx)\n        sequence_id = seq_batch[0][0]\n        seq_num = seq_batch[0][1]\n        seq_df = seq_batch[1]\n        # Extract filepaths and steering angles\n        img_names = seq_df['filepath'].tolist()\n        angles = torch.tensor(seq_df['steering_angle'].tolist(), dtype=torch.float32)\n\n        # Read and process images in one go with OpenCV\n        images = []\n        for img_name in img_names:\n            img = cv2.imread(img_name)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            if self.crop:\n                img = self._crop_lower_half(img)\n            img = cv2.resize(img, (self.imgh, self.imgw ))  # Resize directly with OpenCV\n            images.append(img)\n\n        # Convert to tensor and normalize in batch\n        if self.transform:\n            images = torch.stack([self.transform(img) for img in images])\n\n        return sequence_id, seq_num, images, angles\n    \ndef create_train_val_dataset(train_csv_file, \n                              val_csv_file,\n                              seq_len = 32, \n                              imgw = 224,\n                              imgh = 224,\n                              step_size = 32,\n                              crop = True,\n                              mean=[0.485, 0.456, 0.406],\n                              std=[0.229, 0.224, 0.225]):\n    \n    transform = transforms.Compose([\n        transforms.ToTensor(),  # Convert image to PyTorch tensor\n        transforms.Normalize(mean=mean, std=std)\n    ])\n\n    train_dataset = CustomDataset(csv_file=train_csv_file, seq_len=seq_len, imgh = imgh, imgw=imgw,\n                                  step_size=step_size,crop=crop, transform=transform)\n    val_dataset = CustomDataset(csv_file=val_csv_file, seq_len=seq_len,imgh = imgh, imgw=imgw,\n                                step_size=step_size,crop=crop, transform=transform)\n\n    return train_dataset, val_dataset\n\ndef create_train_val_loader(train_dataset, val_dataset, train_sampler=None, val_sampler=None, batch_size=8,\n                            num_workers=4, prefetch_factor=2, pin_memory=True, train_shuffle=False):\n\n    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size, num_workers=num_workers, \n                              prefetch_factor=prefetch_factor,pin_memory=pin_memory, shuffle=train_shuffle)\n    val_loader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size, num_workers=num_workers, \n                            prefetch_factor=prefetch_factor, pin_memory=pin_memory, shuffle=False)\n\n    print('len of train loader:', len(train_loader))\n    print('len of val loader', len(val_loader))\n\n    for (_, _, inputs, labels) in train_loader:\n        print(\"Batch input shape:\", inputs.shape)\n        print(\"Batch label shape:\", labels.shape)\n        break\n\n    return train_loader, val_loader\n\ndef get_loaders_for_training(data_dir, steering_angles_path, step_size, seq_len, imgh, imgw, filter, turn_threshold, \n                                     buffer_before, buffer_after, crop=True, train_size=0.8, save_dir='data/csv_files', \n                                     norm=True, batch_size=16, num_workers=4, prefetch_factor=4, pin_memory=True, train_shuffle=False):\n    \n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    \n    #preprocesses data\n    data_preprocessed_pd = get_preprocessed_data_pd(data_dir, steering_angles_path, filter, turn_threshold, \n                                     buffer_before, buffer_after, norm, save_dir)\n\n    if filter:\n        train_csv_filename = f\"train_flt_ncp_tt_{turn_threshold}_bb_{buffer_before}_ba_{buffer_after}.csv\"\n        val_csv_filename = f\"val_flt_ncp_tt_{turn_threshold}_bb_{buffer_before}_ba_{buffer_after}.csv\"\n    else:\n        train_csv_filename = f\"train_ncp_tt_{turn_threshold}_bb_{buffer_before}_ba_{buffer_after}.csv\"\n        val_csv_filename = f\"val_ncp_tt_{turn_threshold}_bb_{buffer_before}_ba_{buffer_after}.csv\"\n\n    #splits data into train and val\n    train_dataset_path, val_dataset_path = df_split_train_val(data_preprocessed_pd,\n                                                              train_csv_filename=train_csv_filename,\n                                                              val_csv_filename=val_csv_filename,\n                                                              save_dir=save_dir,\n                                                              train_size=train_size)\n    #gets custom train and test pytorch dataset\n    train_dataset , val_dataset = create_train_val_dataset(train_csv_file = train_dataset_path,\n                                                             val_csv_file = val_dataset_path,\n                                                             seq_len=seq_len, imgh=imgh, imgw=imgw,\n                                                             step_size=step_size, crop=crop)\n    \n    #gets dataloaders from dataset\n    train_loader, val_loader = create_train_val_loader(train_dataset, val_dataset,\n                                                       batch_size=batch_size, \n                                                       num_workers=num_workers, \n                                                       prefetch_factor=prefetch_factor,\n                                                       pin_memory=pin_memory, \n                                                       train_shuffle=train_shuffle)\n\n    return train_loader, val_loader\n\nif __name__ == '__main__':\n\n    #preprocessing csv file args\n    data_dir = '/kaggle/input/sullychen/07012018/data'\n    steering_angles_txt_path = '/kaggle/input/sullychen/07012018/data.txt'\n    save_dir = '/kaggle/working/csv_files_exp'\n    filter = False\n    norm=False\n\n    turn_threshold = 0.06 \n    buffer_before = 32 \n    buffer_after = 32\n    train_size = 0.8\n\n    #custom pytorch dataset args\n    imgh=224\n    imgw=224\n    step_size = 16\n    seq_len = 32\n    crop=True\n\n    #dataloader args\n    batch_size = 16\n    prefetch_factor = 2\n    num_workers=4\n    pin_memory=True\n    train_shuffle=True\n\n    get_loaders_for_training(\n        #preprocessing args:\n        data_dir, steering_angles_path=steering_angles_txt_path, save_dir=save_dir, filter=filter, norm=norm,\n        turn_threshold=turn_threshold, buffer_before=buffer_before, buffer_after=buffer_after, train_size=train_size,\n        #dataset args:\n        imgh=imgh, imgw=imgw, step_size=step_size, seq_len=seq_len, crop=crop, \n        #dataloader args:\n        batch_size=batch_size, prefetch_factor=prefetch_factor, num_workers=num_workers, pin_memory=pin_memory,\n        train_shuffle=train_shuffle)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T19:48:37.717697Z","iopub.execute_input":"2025-04-20T19:48:37.718002Z","iopub.status.idle":"2025-04-20T19:48:53.363106Z","shell.execute_reply.started":"2025-04-20T19:48:37.717983Z","shell.execute_reply":"2025-04-20T19:48:53.362381Z"}},"outputs":[{"name":"stdout","text":"Train dataset length: 51060\nVal dataset length: 12765\n/kaggle/working/csv_files_exp\nTotal sequences extracted: 3190 using step_size=16 and seq_len=32\nTotal sequences extracted: 796 using step_size=16 and seq_len=32\nlen of train loader: 200\nlen of val loader 50\nBatch input shape: torch.Size([16, 32, 3, 224, 224])\nBatch label shape: torch.Size([16, 32])\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Training Script","metadata":{}},{"cell_type":"code","source":"import json\n\nconfig = {\n    \"load_from_ckpt\": False,\n    \"ckpt_save_dir\": \"/kaggle/working/checkpoints/conv_ncp_exp/exp5_0.08_32bb_ss16_fpf16\",\n    \"ckpt_path\": \"\",\n    \"train_dataset_path\": \"/kaggle/working/csv_files_exp/train_flt_ncp_tt_0.08_bb_32_ba_32.csv\",\n    \"val_dataset_path\": \"/kaggle/working/csv_files_exp/val_flt_ncp_tt_0.08_bb_32_ba_32.csv\",\n    \"save_every\": 10,\n    \"epochs\": 50,\n    \"data_dir\": \"/kaggle/input/sullychen/07012018/data\",\n    \"steering_angles_txt_path\": \"/kaggle/input/sullychen/07012018/data.txt\",\n    \"csv_save_dir\": \"/kaggle/working/csv_files_exp\",\n    \"filter\": True,\n    \"norm\": True,\n    \"turn_threshold\": 0.08,\n    \"buffer_before\": 32,\n    \"buffer_after\": 32,\n    \"train_size\": 0.8,\n    \"imgw\": 224,\n    \"imgh\": 224,\n    \"step_size\": 16,\n    \"seq_len\": 32,\n    \"mean\": [0.485, 0.456, 0.406],\n    \"std\": [0.229, 0.224, 0.225],\n    \"crop\": True,\n    \"batch_size\": 32,\n    \"prefetch_factor\": 2,\n    \"num_workers\": 4,\n    \"pin_memory\": True,\n    \"train_shuffle\": True,\n    \"conv_head_lr\": 2.5e-5,\n    \"feat_per_filt\": 16,\n    \"alpha\": 0.1,\n    \"ncp_lr\": 1e-3,\n    \"optim_betas\": [0.9, 0.999],\n    \"he_init\": False\n}\n\nwith open('/kaggle/working/config.json', 'w') as f:\n    json.dump(config, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T01:40:53.731077Z","iopub.execute_input":"2025-04-21T01:40:53.731393Z","iopub.status.idle":"2025-04-21T01:40:53.739645Z","shell.execute_reply.started":"2025-04-21T01:40:53.731369Z","shell.execute_reply":"2025-04-21T01:40:53.738894Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.backends.mps as mps\nimport torch.backends.cudnn as cudnn\nimport os\nimport numpy as np\nimport cv2\nimport json\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\ndef plot_loss_accuracy(train_loss, val_loss, save_dir=None):\n    epochs = range(1, len(train_loss) + 1)\n\n    plt.figure(figsize=(12, 6))\n\n    # Plot Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_loss, label='Train Loss', color='blue', linestyle='-')\n    plt.plot(epochs, val_loss, label='Validation Loss', color='red', linestyle='--')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Loss vs Epochs')\n    plt.tight_layout()\n    if save_dir:\n        plt.savefig(save_dir, bbox_inches='tight')\n        plt.close()\n    else:\n        plt.show()\n\ndef overlay_visual_backprop(input_tensor, mask, save_path=None, alpha=0.1):\n    \"\"\"\n    Overlays the visual backprop mask on the original input image.\n\n    Args:\n        input_tensor: [3, H, W] torch.Tensor (before batch dimension), normalized\n        mask: [H, W] numpy array, already normalized [0, 1]\n        save_path: optional path to save overlay image\n        alpha: blending factor (heatmap vs original)\n    \"\"\"\n    #denormalize image (undo mean/std normalization)\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    img = input_tensor.clone().detach().cpu().numpy() \n    img = img * std[:, None, None] + mean[:, None, None]\n    img = np.clip(img, 0, 1)\n    img = np.transpose(img, (1, 2, 0))  # -> [H, W, 3]\n\n    if mask.shape != img.shape[:2]:\n        mask = cv2.resize(mask, (img.shape[1], img.shape[0]))\n\n    #colormap to mask and overlay\n    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB) / 255.0\n    overlayed = (1 - alpha) * img + alpha * heatmap\n    overlayed = np.clip(overlayed, 0, 1)\n\n    plt.imshow(overlayed)\n    plt.axis('off')\n    if save_path:\n        plt.savefig(save_path, bbox_inches='tight')\n        plt.close()\n    else:\n        plt.show()\n\ndef train_validate(train_loader, val_loader, optimizer, model, criterion, train_params, current_epoch=0, epochs=10, \n                   save_dir = 'checkpoints/', training_losses = [], validation_losses = [], save_every=10):\n\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    backprop_save_dir = os.path.join(save_dir,'backprops')\n    if not os.path.exists(backprop_save_dir):\n        os.makedirs(backprop_save_dir)\n\n    this_run_epoch = 0\n    for epoch in range(current_epoch, epochs): \n        model.train()\n        total_train_loss = 0.0\n        for _, (_, _, batch_x, batch_y) in tqdm(enumerate(train_loader), \n                                          desc=f'Training {epoch+1}/{epochs}:', total=len(train_loader), ncols=100):\n            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n\n            optimizer.zero_grad()\n            predictions = model(batch_x)\n            loss = criterion(predictions, batch_y)\n\n            total_train_loss += loss.item()\n\n            loss.backward()\n            optimizer.step()\n\n        training_losses.append(total_train_loss/len(train_loader))\n\n        print(f\"Train Loss: {total_train_loss/len(train_loader)}\")\n\n        #validation loop\n        model.eval()\n        total_val_loss = 0.0\n        for _, (_, _, batch_x, batch_y) in tqdm(enumerate(val_loader), \n                                          desc=f'Validation {epoch+1}/{epochs}:', total=len(val_loader), ncols=100):\n            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n\n            with torch.no_grad():\n                predictions = model(batch_x)\n                loss = criterion(predictions, batch_y)\n\n            total_val_loss += loss.item()\n\n        validation_losses.append(total_val_loss/ len(val_loader))\n\n        print(f\"Validation Loss: {total_val_loss / len(val_loader)}\")\n        \n        # visualbackprop dump\n        with torch.no_grad():\n            model.eval()\n            batch = next(iter(train_loader))\n            _, _, batch_x, _ = batch\n            batch_x = batch_x.to(device)\n\n            B, T, C, H, W = batch_x.shape\n            x_flat = batch_x.view(B*T, C, H, W)\n\n            _ = model.conv_head(x_flat)  # intermediate activations\n            vis_mask = model.conv_head.visual_backprop(idx=0)\n            input_image = x_flat[0]  # one image: shape [3, H, W]\n            overlay_visual_backprop(input_image, vis_mask, save_path=f'{backprop_save_dir}/epoch_{epoch+1}.png', alpha=0.5)\n            plt.close()\n\n        this_run_epoch += 1\n        if this_run_epoch % save_every  == 0:\n\n            checkpoint = {\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'epoch': epoch + 1,\n            'training_losses': training_losses,\n            'validation_losses': validation_losses,\n            'train_params': train_params,\n            }\n\n            model_path = os.path.join(save_dir, f'model_epoch{epoch+1}.pth')\n            torch.save(checkpoint, model_path)\n            print(f\"Checkpoint saved to {save_dir}\\n\")\n\n    return model_path\n\ndef init_weights_he(m):\n    if isinstance(m, (nn.Linear, nn.Conv2d)):\n        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\n\ndef load_config(config_path='/kaggle/working/config.json'):\n    with open(config_path, 'r') as f:\n        config = json.load(f)\n    return config\n\nif __name__ == '__main__':\n\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n        cudnn.benchmark = True\n        print(f'Using CUDA device: {torch.cuda.get_device_name(0)}')\n    elif torch.backends.mps.is_available():\n        device = torch.device('mps')\n        mps.benchmark = True\n        print(\"Using MPS device\")\n    else:\n        device = torch.device('cpu')\n        print(\"Using CPU\")\n\n    config = load_config()\n\n    train_loader, val_loader = get_loaders_for_training(\n    # Preprocessing args:\n    data_dir=config[\"data_dir\"], steering_angles_path=config[\"steering_angles_txt_path\"], save_dir=config[\"csv_save_dir\"],\n    filter=config[\"filter\"], norm=config[\"norm\"], turn_threshold=config[\"turn_threshold\"], buffer_before=config[\"buffer_before\"],\n    buffer_after=config[\"buffer_after\"], train_size=config[\"train_size\"],\n\n    # Dataset args:\n    imgh=config[\"imgh\"], imgw=config[\"imgw\"], step_size=config[\"step_size\"], seq_len=config[\"seq_len\"], crop=config[\"crop\"],\n\n    # Dataloader args:\n    batch_size=config[\"batch_size\"], prefetch_factor=config[\"prefetch_factor\"], num_workers=config[\"num_workers\"], \n    pin_memory=config[\"pin_memory\"], train_shuffle=config[\"train_shuffle\"])\n\n    \n    # Assuming extracted features from conv head (8*4) are 32-dimensional\n    model = ConvNCPModel(num_filters=8, features_per_filter=config['feat_per_filt'], inter_neurons = 12,\n                        command_neurons = 6, motor_neurons = 1, sensory_fanout = 6, inter_fanout = 4, \n                        recurrent_command_synapses = 6, motor_fanin = 6, seed = 20190120) \n    \n    if config['he_init']:\n        model.apply(init_weights_he)\n        \n    model = model.to(device)\n\n    # Define loss function and optimizer\n    criterion = WeightedMSE(config['alpha'])\n    # criterion = nn.MSELoss()\n    optimizer = optim.Adam([\n    # Convolutional head\n    {'params': model.conv_head.parameters(), 'lr': config['conv_head_lr']},\n    # NCP/LTC\n    {'params': model.ltc.parameters(), 'lr': config['ncp_lr']},\n    # Output layer\n    {'params': model.fc_out.parameters(), 'lr': config['ncp_lr']}], \n        betas=config['optim_betas'])\n\n    if config['load_from_ckpt']:\n        model_ckpt = torch.load(config['ckpt_path'], map_location=device)\n        model.load_state_dict(model_ckpt['model_state_dict'])\n        optimizer.load_state_dict(model_ckpt['optimizer_state_dict'])\n        current_epoch = model_ckpt['epoch']\n        training_losses = model_ckpt['training_losses']\n        validation_losses = model_ckpt['validation_losses']\n        loaded_train_params = model_ckpt['train_params']\n        print('checkpoint loaded successfully!')\n\n    else:\n            current_epoch = 0\n            training_losses = []\n            validation_losses = []\n    \n    if len(training_losses) > 0:\n        print(\"last training and validation losses:\", training_losses[-1], validation_losses[-1])\n    else:\n        print('Training losses:', training_losses)\n        print('Validation losses:', validation_losses)\n    print('Current Epoch Number:', current_epoch)\n\n    final_model_path = train_validate(train_loader=train_loader,\n          val_loader=val_loader,\n          optimizer=optimizer,\n          model=model,\n          train_params=config,\n          criterion=criterion,\n          current_epoch=current_epoch,\n          epochs=config['epochs'], \n          save_dir=config['ckpt_save_dir'],\n          training_losses=training_losses,\n          validation_losses=validation_losses,\n          save_every=config['save_every'])\n    \n    final_checkpoint = torch.load(final_model_path)\n\n    training_losses = final_checkpoint['training_losses']\n    validation_losses = final_checkpoint['validation_losses']\n\n    plot_loss_accuracy(training_losses, validation_losses, save_dir=config['ckpt_save_dir'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T01:41:00.470481Z","iopub.execute_input":"2025-04-21T01:41:00.471033Z","iopub.status.idle":"2025-04-21T02:05:37.981871Z","shell.execute_reply.started":"2025-04-21T01:41:00.471011Z","shell.execute_reply":"2025-04-21T02:05:37.980702Z"}},"outputs":[{"name":"stdout","text":"Using CUDA device: Tesla T4\nMinimum sequence length: 77\nMaximum sequence length: 539\nTotal valid sequences: 55\nTrain dataset length: 8791\nVal dataset length: 2198\n/kaggle/working/csv_files_exp\nTotal sequences extracted: 486 using step_size=16 and seq_len=32\nTotal sequences extracted: 116 using step_size=16 and seq_len=32\nlen of train loader: 16\nlen of val loader 4\nBatch input shape: torch.Size([32, 32, 3, 224, 224])\nBatch label shape: torch.Size([32, 32])\nTraining losses: []\nValidation losses: []\nCurrent Epoch Number: 0\n","output_type":"stream"},{"name":"stderr","text":"Training 1/50:: 100%|███████████████████████████████████████████████| 16/16 [00:27<00:00,  1.75s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.4888116344809532\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 1/50:: 100%|███████████████████████████████████████████████| 4/4 [00:07<00:00,  1.89s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.3819001838564873\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 2/50:: 100%|███████████████████████████████████████████████| 16/16 [00:26<00:00,  1.65s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.40255807153880596\n","output_type":"stream"},{"name":"stderr","text":"Validation 2/50:: 100%|███████████████████████████████████████████████| 4/4 [00:07<00:00,  1.89s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.30517419427633286\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 3/50:: 100%|███████████████████████████████████████████████| 16/16 [00:28<00:00,  1.76s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.32814805768430233\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 3/50:: 100%|███████████████████████████████████████████████| 4/4 [00:07<00:00,  1.89s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.23832038789987564\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 4/50:: 100%|███████████████████████████████████████████████| 16/16 [00:28<00:00,  1.75s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.25109065789729357\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 4/50:: 100%|███████████████████████████████████████████████| 4/4 [00:07<00:00,  1.96s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.17815269902348518\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 5/50:: 100%|███████████████████████████████████████████████| 16/16 [00:29<00:00,  1.82s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1918056532740593\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 5/50:: 100%|███████████████████████████████████████████████| 4/4 [00:07<00:00,  1.89s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.13516053929924965\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 6/50:: 100%|███████████████████████████████████████████████| 16/16 [00:27<00:00,  1.75s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.15188538189977407\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 6/50:: 100%|███████████████████████████████████████████████| 4/4 [00:07<00:00,  1.96s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.10329722426831722\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 7/50:: 100%|███████████████████████████████████████████████| 16/16 [00:28<00:00,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.11727162217721343\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 7/50:: 100%|███████████████████████████████████████████████| 4/4 [00:07<00:00,  1.83s/it]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.07907368429005146\n","output_type":"stream"},{"name":"stderr","text":"Training 8/50:: 100%|███████████████████████████████████████████████| 16/16 [00:28<00:00,  1.78s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.09264656249433756\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 8/50:: 100%|███████████████████████████████████████████████| 4/4 [00:07<00:00,  1.84s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.061014700680971146\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 9/50:: 100%|███████████████████████████████████████████████| 16/16 [00:28<00:00,  1.77s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.07249165256507695\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 9/50:: 100%|███████████████████████████████████████████████| 4/4 [00:07<00:00,  1.75s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.04848985932767391\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 10/50:: 100%|██████████████████████████████████████████████| 16/16 [00:28<00:00,  1.79s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.059347369242459536\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 10/50:: 100%|██████████████████████████████████████████████| 4/4 [00:07<00:00,  1.93s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.04031631117686629\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved to /kaggle/working/checkpoints/conv_ncp_exp/exp5_0.08_32bb_ss16_fpf16\n\n","output_type":"stream"},{"name":"stderr","text":"Training 11/50:: 100%|██████████████████████████████████████████████| 16/16 [00:27<00:00,  1.75s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.051698652910999954\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 11/50:: 100%|██████████████████████████████████████████████| 4/4 [00:07<00:00,  1.93s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.035405082860961556\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 12/50:: 100%|██████████████████████████████████████████████| 16/16 [00:27<00:00,  1.70s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.04265775578096509\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 12/50:: 100%|██████████████████████████████████████████████| 4/4 [00:07<00:00,  1.76s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.03302645101211965\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 13/50:: 100%|██████████████████████████████████████████████| 16/16 [00:26<00:00,  1.66s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.03870675002690405\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 13/50:: 100%|██████████████████████████████████████████████| 4/4 [00:07<00:00,  1.78s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.03224399173632264\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 14/50:: 100%|██████████████████████████████████████████████| 16/16 [00:26<00:00,  1.66s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.03718192514497787\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 14/50:: 100%|██████████████████████████████████████████████| 4/4 [00:06<00:00,  1.73s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.0322459468152374\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 15/50:: 100%|██████████████████████████████████████████████| 16/16 [00:26<00:00,  1.67s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.03836087021045387\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 15/50:: 100%|██████████████████████████████████████████████| 4/4 [00:06<00:00,  1.73s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.03264010697603226\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 16/50:: 100%|██████████████████████████████████████████████| 16/16 [00:25<00:00,  1.61s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.037699098931625485\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 16/50:: 100%|██████████████████████████████████████████████| 4/4 [00:06<00:00,  1.71s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.03304972080513835\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 17/50:: 100%|██████████████████████████████████████████████| 16/16 [00:27<00:00,  1.69s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.03433856804622337\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 17/50:: 100%|██████████████████████████████████████████████| 4/4 [00:06<00:00,  1.72s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.03339187055826187\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 18/50:: 100%|██████████████████████████████████████████████| 16/16 [00:26<00:00,  1.67s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.03442566329613328\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 18/50:: 100%|██████████████████████████████████████████████| 4/4 [00:07<00:00,  1.82s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.03357815649360418\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 19/50:: 100%|██████████████████████████████████████████████| 16/16 [00:26<00:00,  1.68s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.034266244794707745\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 19/50:: 100%|██████████████████████████████████████████████| 4/4 [00:07<00:00,  1.81s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.033637679647654295\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 20/50:: 100%|██████████████████████████████████████████████| 16/16 [00:26<00:00,  1.66s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.03838872816413641\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 20/50:: 100%|██████████████████████████████████████████████| 4/4 [00:07<00:00,  1.88s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.033744022250175476\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved to /kaggle/working/checkpoints/conv_ncp_exp/exp5_0.08_32bb_ss16_fpf16\n\n","output_type":"stream"},{"name":"stderr","text":"Training 21/50:: 100%|██████████████████████████████████████████████| 16/16 [00:26<00:00,  1.63s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.03465953574050218\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 21/50:: 100%|██████████████████████████████████████████████| 4/4 [00:06<00:00,  1.71s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.033483573934063315\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 22/50:: 100%|██████████████████████████████████████████████| 16/16 [00:26<00:00,  1.65s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.034837951650843024\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 22/50:: 100%|██████████████████████████████████████████████| 4/4 [00:06<00:00,  1.70s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.03353830939158797\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 23/50:: 100%|██████████████████████████████████████████████| 16/16 [00:27<00:00,  1.71s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0353165838168934\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 23/50:: 100%|██████████████████████████████████████████████| 4/4 [00:07<00:00,  1.80s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.03343341359868646\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 24/50:: 100%|██████████████████████████████████████████████| 16/16 [00:27<00:00,  1.74s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.03464849619194865\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 24/50:: 100%|██████████████████████████████████████████████| 4/4 [00:07<00:00,  1.83s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.03339860402047634\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 25/50:: 100%|██████████████████████████████████████████████| 16/16 [00:27<00:00,  1.70s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.033631901344051585\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 25/50:: 100%|██████████████████████████████████████████████| 4/4 [00:07<00:00,  1.84s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.033495324198156595\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 26/50:: 100%|██████████████████████████████████████████████| 16/16 [00:27<00:00,  1.71s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.03555608540773392\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 26/50:: 100%|██████████████████████████████████████████████| 4/4 [00:07<00:00,  1.76s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.03351765451952815\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 27/50:: 100%|██████████████████████████████████████████████| 16/16 [00:28<00:00,  1.78s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.03438604937400669\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 27/50:: 100%|██████████████████████████████████████████████| 4/4 [00:07<00:00,  1.81s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.033464611042290926\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 28/50:: 100%|██████████████████████████████████████████████| 16/16 [00:27<00:00,  1.71s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.03421174769755453\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 28/50:: 100%|██████████████████████████████████████████████| 4/4 [00:07<00:00,  1.79s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.033304489916190505\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 29/50:: 100%|██████████████████████████████████████████████| 16/16 [00:26<00:00,  1.67s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.03400142549071461\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 29/50:: 100%|██████████████████████████████████████████████| 4/4 [00:07<00:00,  1.81s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.03304178989492357\n","output_type":"stream"},{"name":"stderr","text":"\nTraining 30/50:: 100%|██████████████████████████████████████████████| 16/16 [00:26<00:00,  1.67s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.03470766940154135\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 30/50:: 100%|██████████████████████████████████████████████| 4/4 [00:07<00:00,  1.79s/it]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.03288056259043515\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Checkpoint saved to /kaggle/working/checkpoints/conv_ncp_exp/exp5_0.08_32bb_ss16_fpf16\n\n","output_type":"stream"},{"name":"stderr","text":"Training 31/50:: 100%|██████████████████████████████████████████████| 16/16 [00:26<00:00,  1.69s/it]","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.03587806364521384\n","output_type":"stream"},{"name":"stderr","text":"\nValidation 31/50::  25%|███████████▌                                  | 1/4 [00:06<00:19,  6.53s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1920678926.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Current Epoch Number:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     final_model_path = train_validate(train_loader=train_loader,\n\u001b[0m\u001b[1;32m    236\u001b[0m           \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/1920678926.py\u001b[0m in \u001b[0;36mtrain_validate\u001b[0;34m(train_loader, val_loader, optimizer, model, criterion, train_params, current_epoch, epochs, save_dir, training_losses, validation_losses, save_every)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/409017682.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# forward pass through LTC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mltc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch, seq_len, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# map NCP output to steering angle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ncps/torch/ltc.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx, timespans)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_mixed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mh_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mh_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0moutput_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ncps/torch/ltc_cell.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, states, elapsed_time)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ode_solver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melapsed_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ncps/torch/ltc_cell.py\u001b[0m in \u001b[0;36m_ode_solver\u001b[0;34m(self, inputs, state, elapsed_time)\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mw_denominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_activation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw_denominator_sensory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0mgleak\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_positive_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gleak\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0mnumerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcm_t\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv_pre\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgleak\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vleak\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw_numerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcm_t\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgleak\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw_denominator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":44}]}