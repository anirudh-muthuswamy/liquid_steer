{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11382018,"sourceType":"datasetVersion","datasetId":7126841},{"sourceId":11520873,"sourceType":"datasetVersion","datasetId":7225491}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch\nimport pandas as pd\nimport re\nimport cv2\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm\nfrom collections import OrderedDict\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport torch.nn.functional as F\nfrom torchinfo import summary","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:51:51.701572Z","iopub.execute_input":"2025-04-23T17:51:51.701793Z","iopub.status.idle":"2025-04-23T17:51:59.082435Z","shell.execute_reply.started":"2025-04-23T17:51:51.701770Z","shell.execute_reply":"2025-04-23T17:51:59.081843Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"pd.set_option(\"display.max_rows\", 200)\nplt.ion() \n\ndef get_full_image_filepaths(data_dir):\n    filepaths = os.listdir(data_dir)\n    sorted_filepaths = sorted(filepaths, key=lambda x: int(re.search(r'\\d+', x).group()))\n    full_filepaths = [os.path.join(data_dir, file) for file in sorted_filepaths]\n\n    return full_filepaths\n\ndef get_steering_angles(path):\n    file = open(path, 'r')\n    lines = file.readlines()\n    #change this to line[:-1] if using kaggle dataset else keep the same for sullychen dataset\n    steering_angles = [(line[:-1]).split(' ')[1].split(',')[0] for line in lines]\n    timestamps = [line.strip().split(' ', 1)[1].split(',')[1] for line in lines]\n\n    return [steering_angles, timestamps]\n\ndef convert_to_df(full_filepaths, steering_angles, timestamps, norm=True):\n    data = pd.DataFrame({'filepath':full_filepaths,'steering_angle':steering_angles, 'timestamps':timestamps})\n    data['parsed_timestamp'] = data['timestamps'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S:%f'))\n    data = data.drop('timestamps', axis=1)\n    data = data.reset_index(drop=True)\n    data['steering_angle'] = data['steering_angle'].astype('float')\n    #normalize steering angles to -1,1 based on actual steering range (-360, 360) of wheel\n    if norm == True:\n        data['steering_angle'] = data['steering_angle'] / 360\n    return data\n\ndef display_images_with_angles(data_pd, steering_wheel_img):\n\n    # steering wheel image\n    img = cv2.imread(steering_wheel_img,0)\n    rows,cols = img.shape\n    i = 0\n    xs = data_pd['filepath'].values\n    ys = data_pd['steering_angle'].values\n\n    #Press q to stop \n    while(cv2.waitKey(100) != ord('q')) and i < len(xs):\n        try:\n            # Driving Test Image displayed as Video\n            full_image = cv2.imread(xs[i])\n\n            degrees = ys[i] * 360\n            print(\"Steering angle: \" + str(degrees) + \" (actual)\")\n            cv2.imshow(\"frame\", full_image)\n\n            # Angle at which the steering wheel image should be rotated\n            M = cv2.getRotationMatrix2D((cols/2,rows/2),-degrees,1)\n            dst = cv2.warpAffine(img,M,(cols,rows))\n\n            cv2.imshow(\"steering wheel\", dst)\n        except:\n            print('ERROR at', i)\n        i += 1\n    cv2.destroyAllWindows()\n\ndef disp_freq_steering_angles(data_pd):\n    # Define bin edges from -1 to 1 with step 0.1\n    bin_edges = list(range(-10, 11))  # Since steering angle is between -1 and 1, multiply by 10\n    bin_edges = [x / 10 for x in bin_edges]  # Convert back to decimal values\n\n    # Assign steering angles to bins\n    data_pd['binned'] = pd.cut(data_pd['steering_angle'].astype('float'), bins=bin_edges, right=False)\n\n    # Count occurrences in each bin\n    bin_counts = data_pd['binned'].value_counts().sort_index()\n\n    # Plot bar chart\n    plt.figure(figsize=(12, 5))\n    bin_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n\n    # Formatting\n    plt.xlabel(\"Steering Angle Range\")\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Frequency of Steering Angles in 0.1 Intervals\")\n    plt.xticks(rotation=45)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n\n    plt.show()\n\ndef disp_start_and_end_in_filtered_data(data_pd_filtered):\n    turn_starts = data_pd_filtered[data_pd_filtered['turn_shift'] == 1]\n    turn_ends = data_pd_filtered[data_pd_filtered['turn_shift'] == -1]\n\n    plt.figure(figsize=(12, 5))\n    plt.plot(data_pd_filtered.index, data_pd_filtered['steering_angle'], label=\"Steering Angle\", alpha=0.5)\n    plt.scatter(turn_starts.index, turn_starts['steering_angle'], color='red', label=\"Turn Start\", marker=\"o\")\n    plt.scatter(turn_ends.index, turn_ends['steering_angle'], color='blue', label=\"Turn End\", marker=\"x\")\n\n    plt.xlabel(\"Frame Index\")\n    plt.ylabel(\"Steering Angle\")\n    plt.title(\"Turn Start and End Points in Steering Angle Data\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef filter_df_on_turns(data_pd, turn_threshold = 0.06, buffer_before = 60, buffer_after = 60):\n    # Parameters\n    turn_threshold = turn_threshold  # Define turn threshold (absolute value)\n    buffer_before = buffer_before    # Frames to include before a turn\n    buffer_after = buffer_after     # Frames to include after a turn\n\n    # Load your dataset (assuming it's a DataFrame named df)\n    data_pd['index'] = data_pd.index  # Preserve original ordering if needed\n\n    # Identify where turning happens\n    data_pd['turning'] = (data_pd['steering_angle'].abs() > turn_threshold).astype(int)\n\n    # Find where turns start and end\n    data_pd['turn_shift'] = data_pd['turning'].diff()  # 1 indicates start, -1 indicates end\n\n    # Get turn start and end indices\n    turn_starts = data_pd[data_pd['turn_shift'] == 1].index\n    turn_ends = data_pd[data_pd['turn_shift'] == -1].index\n\n    # Ensure equal number of start and end points\n    if len(turn_ends) > 0 and turn_starts[0] > turn_ends[0]:  \n        turn_ends = turn_ends[1:]  # Drop the first turn_end if it comes before a start\n\n    # Selected indices for keeping\n    selected_indices = set()\n\n    for start, end in zip(turn_starts, turn_ends):\n        # Include a buffer of frames before and after\n        start_idx = max(0, start - buffer_before)\n        end_idx = min(len(data_pd) - 1, end + buffer_after)\n        \n        # Add indices to selection\n        selected_indices.update(range(start_idx, end_idx + 1))\n\n    # Create filtered dataset\n    data_pd_filtered = data_pd.loc[sorted(selected_indices)].reset_index(drop=True)\n\n    # Drop temporary columns\n    data_pd_filtered = data_pd_filtered.drop(columns=['turning', 'turn_shift'])\n\n    # Detect sequence breaks (where the original index is not continuous)\n    data_pd_filtered[\"sequence_id\"] = (data_pd_filtered[\"index\"].diff() != 1).cumsum()\n\n    return data_pd_filtered\n\ndef group_data_by_sequences(data_pd_filtered):\n    sequence_lengths = data_pd_filtered.groupby(\"sequence_id\").size()\n\n    print(f\"Minimum sequence length: {min(sequence_lengths)}\")\n    print(f\"Maximum sequence length: {max(sequence_lengths)}\")\n\n    # plt.plot(sequence_lengths)\n\n    # Keep only sequences with at least 10 frames (adjust as needed)\n    valid_sequences = sequence_lengths[sequence_lengths >= 40].index\n    data_pd_filtered = data_pd_filtered[data_pd_filtered[\"sequence_id\"].isin(valid_sequences)]\n\n    print(f\"Total valid sequences: {len(valid_sequences)}\")\n\n    return data_pd_filtered\n\ndef get_preprocessed_data_pd(data_dir, steering_angles_txt_path, filter = True,\n                             turn_threshold = 0.06, buffer_before = 60, buffer_after = 60,\n                             norm=True, save_dir = 'data/csv_files'):\n    img_paths = get_full_image_filepaths(data_dir)\n    steering_angles, timestamps = get_steering_angles(steering_angles_txt_path)\n\n    data_pd = convert_to_df(img_paths, steering_angles, timestamps, norm)\n    if filter and norm:\n        data_pd_filtered = filter_df_on_turns(data_pd, turn_threshold = turn_threshold, \n                                            buffer_before = buffer_before, buffer_after = buffer_after)\n        data_pd_filtered = group_data_by_sequences(data_pd_filtered)\n\n        # Save\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        data_pd_filtered.to_csv(os.path.join(save_dir,f\"flt_ncp_tt_{turn_threshold}_bb_{buffer_before}_ba_{buffer_after}.csv\"), index=False)\n\n        return data_pd_filtered\n    \n    elif filter and not norm:\n        print('Error: If filtering, then steering values should be normalized (-1, 1), set norm to True')\n        exit(1)\n\n    else:\n        sequence_id = 0\n        sequence_ids = [sequence_id]\n\n        # Iterate through rows to calculate time differences (if greater than 3 seconds \n        # or not) and assign sequence IDs\n        for i in range(1, len(data_pd)):\n            time_diff = (data_pd['parsed_timestamp'][i] - data_pd['parsed_timestamp'][i-1]).total_seconds()\n            if time_diff > 3:\n                sequence_id += 1\n            sequence_ids.append(sequence_id)\n\n        # Add sequence_id column to DataFrame\n        data_pd['sequence_id'] = sequence_ids\n\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        data_pd.to_csv(os.path.join(save_dir,f\"ncp_unfiltered.csv\"), index=False)\n        \n        return data_pd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:51:59.083525Z","iopub.execute_input":"2025-04-23T17:51:59.083901Z","iopub.status.idle":"2025-04-23T17:51:59.104028Z","shell.execute_reply.started":"2025-04-23T17:51:59.083872Z","shell.execute_reply":"2025-04-23T17:51:59.103360Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def df_split_train_val(df_filtered, train_csv_filename, val_csv_filename,\n                       save_dir='data/csv_files',train_size = 0.8):\n    train_dataset = df_filtered[:int(train_size * len(df_filtered))]\n    val_dataset = df_filtered[int(train_size * len(df_filtered)):]\n    print('Train dataset length:', len(train_dataset))\n    print('Val dataset length:', len(val_dataset))\n    print(save_dir)\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    train_dataset.to_csv(os.path.join(save_dir,train_csv_filename), index=False)\n    val_dataset.to_csv(os.path.join(save_dir,val_csv_filename), index=False)\n\n    return os.path.join(save_dir,train_csv_filename), os.path.join(save_dir,val_csv_filename)\n\ndef calculate_mean_and_std(dataset_path):\n    num_pixels = 0\n    channel_sum = np.zeros(3)  # Assuming RGB images, change to (1,) for grayscale\n    channel_sum_squared = np.zeros(3)  # Assuming RGB images, change to (1,) for grayscale\n\n    for root, _, files in os.walk(dataset_path):\n        for file in files:\n            image_path = os.path.join(root, file)\n            image = Image.open(image_path).convert('RGB')  # Convert to RGB if needed\n\n            pixels = np.array(image) / 255.0  # Normalize pixel values between 0 and 1\n            num_pixels += pixels.size // 3  # Assuming RGB images, change to 1 for grayscale\n\n            channel_sum += np.sum(pixels, axis=(0, 1))\n            channel_sum_squared += np.sum(pixels ** 2, axis=(0, 1))\n\n    mean = channel_sum / num_pixels\n    std = np.sqrt((channel_sum_squared / num_pixels) - mean ** 2)\n    return mean, std\n\nclass CustomDataset(Dataset):\n    def __init__(self,\n                 csv_file,\n                 seq_len,\n                 imgh=224,\n                 imgw=224,\n                 step_size=1,\n                 crop=True,\n                 transform=None):\n\n        self.df        = pd.read_csv(csv_file)\n        self.seq_len   = seq_len\n        self.imgh      = imgh\n        self.imgw      = imgw\n        self.step_size = step_size\n        self.crop      = crop\n        self.transform = transform\n\n        self.sequences = []  # will hold tuples (seq_df, next_angle)\n\n        # group once by sequence_id\n        for seq_id, seq_data in self.df.groupby(\"sequence_id\"):\n            # we need at least seq_len + 1 frames to form one training example\n            N = len(seq_data)\n            for start in range(0, N - seq_len, step_size):\n                window = seq_data.iloc[start : start + seq_len]\n                next_angle = seq_data.iloc[start + seq_len][\"steering_angle\"]\n                self.sequences.append((window.reset_index(drop=True), next_angle))\n\n        print(f\"Total examples: {len(self.sequences)} \"\n              f\"(each is {seq_len} frames → 1 target)\")\n\n    def _crop_lower_half(self, img, keep_ratio=0.6):\n        h = img.shape[0]\n        return img[int(h*(1-keep_ratio)) :, :, :]\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        seq_df, next_angle = self.sequences[idx]\n\n        imgs = []\n        for fp in seq_df[\"filepath\"]:\n            img = np.expand_dims(cv2.imread(fp, cv2.IMREAD_GRAYSCALE),2)\n            if self.crop:\n                img = self._crop_lower_half(img)\n            img = cv2.resize(img, (self.imgh, self.imgw))\n            if self.transform:\n                img = self.transform(img)  # → C×H×W\n            imgs.append(img)\n\n        # shape (seq_len, C, H, W)\n        x = torch.stack(imgs, dim=0).permute(1, 0, 2, 3)\n        y = torch.tensor([next_angle], dtype=torch.float32)\n        return x, y\n    \n\ndef create_train_val_dataset(train_csv_file, \n                              val_csv_file,\n                              seq_len = 32, \n                              imgw = 224,\n                              imgh = 224,\n                              step_size = 32,\n                              crop = True,\n                              mean=[0.485, 0.456, 0.406],\n                              std=[0.229, 0.224, 0.225]):\n    \n    transform = transforms.Compose([\n        transforms.ToTensor(),  # Convert image to PyTorch tensor\n        transforms.Normalize(mean=mean, std=std)\n    ])\n\n    train_dataset = CustomDataset(csv_file=train_csv_file, seq_len=seq_len, imgh = imgh, imgw=imgw,\n                                  step_size=step_size,crop=crop, transform=transform)\n    val_dataset = CustomDataset(csv_file=val_csv_file, seq_len=seq_len,imgh = imgh, imgw=imgw,\n                                step_size=step_size,crop=crop, transform=transform)\n\n    return train_dataset, val_dataset\n\ndef create_train_val_loader(train_dataset, val_dataset, train_sampler=None, val_sampler=None, batch_size=8,\n                            num_workers=4, prefetch_factor=2, pin_memory=True, train_shuffle=False):\n\n    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size, num_workers=num_workers, \n                              prefetch_factor=prefetch_factor,pin_memory=pin_memory, shuffle=train_shuffle)\n    val_loader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size, num_workers=num_workers, \n                            prefetch_factor=prefetch_factor, pin_memory=pin_memory, shuffle=False)\n\n    print('len of train loader:', len(train_loader))\n    print('len of val loader', len(val_loader))\n\n    for (inputs, labels) in train_loader:\n        print(\"Batch input shape:\", inputs.shape)\n        print(\"Batch label shape:\", labels.shape)\n        break\n\n    return train_loader, val_loader\n\ndef get_loaders_for_training(data_dir, steering_angles_path, step_size, seq_len, imgh, imgw, filter, turn_threshold, \n                                     buffer_before, buffer_after, crop=True, train_size=0.8, save_dir='data/csv_files', \n                                     std=[0.5], mean=[0.5],\n                                     norm=True, batch_size=16, num_workers=4, prefetch_factor=4, pin_memory=True, train_shuffle=False):\n    \n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    \n    #preprocesses data\n    data_preprocessed_pd = get_preprocessed_data_pd(data_dir, steering_angles_path, filter, turn_threshold, \n                                     buffer_before, buffer_after, norm, save_dir)\n\n    if filter:\n        train_csv_filename = f\"train_flt_ncp_tt_{turn_threshold}_bb_{buffer_before}_ba_{buffer_after}.csv\"\n        val_csv_filename = f\"val_flt_ncp_tt_{turn_threshold}_bb_{buffer_before}_ba_{buffer_after}.csv\"\n    else:\n        train_csv_filename = f\"train_ncp_unfiltered.csv\"\n        val_csv_filename = f\"val_ncp_unfiltered.csv\"\n\n    #splits data into train and val\n    train_dataset_path, val_dataset_path = df_split_train_val(data_preprocessed_pd,\n                                                              train_csv_filename=train_csv_filename,\n                                                              val_csv_filename=val_csv_filename,\n                                                              save_dir=save_dir,\n                                                              train_size=train_size)\n    #gets custom train and test pytorch dataset\n    train_dataset , val_dataset = create_train_val_dataset(train_csv_file = train_dataset_path,\n                                                             val_csv_file = val_dataset_path,\n                                                             seq_len=seq_len, imgh=imgh, imgw=imgw,\n                                                             step_size=step_size, crop=crop,\n                                                             std=std,\n                                                             mean=mean)\n    \n    #gets dataloaders from dataset\n    train_loader, val_loader = create_train_val_loader(train_dataset, val_dataset,\n                                                       batch_size=batch_size, \n                                                       num_workers=num_workers, \n                                                       prefetch_factor=prefetch_factor,\n                                                       pin_memory=pin_memory, \n                                                       train_shuffle=train_shuffle)\n\n    return train_loader, val_loader\n\nif __name__ == '__main__':\n\n    #preprocessing csv file args\n    data_dir = '/kaggle/input/sullychen-edgemaps/edge_maps/edge_maps'\n    steering_angles_txt_path = '/kaggle/input/sullychen/07012018/data.txt'\n    save_dir = '/kaggle/working/convnet-edgemap'\n    filter = True\n    norm=True\n\n    turn_threshold = 0.08\n    buffer_before = 32\n    buffer_after = 32\n    train_size = 0.8\n\n    #custom pytorch dataset args\n    imgh=224\n    imgw=224\n    step_size = 1\n    seq_len = 8\n    crop=False\n\n    #dataloader args\n    batch_size = 16\n    prefetch_factor = 2\n    num_workers=4\n    pin_memory=True\n    train_shuffle=False\n\n    get_loaders_for_training(\n        #preprocessing args:\n        data_dir, steering_angles_path=steering_angles_txt_path, save_dir=save_dir, filter=filter, norm=norm,\n        turn_threshold=turn_threshold, buffer_before=buffer_before, buffer_after=buffer_after, train_size=train_size,\n        #dataset args:\n        imgh=imgh, imgw=imgw, step_size=step_size, seq_len=seq_len, crop=crop, \n        #dataloader args:\n        batch_size=batch_size, prefetch_factor=prefetch_factor, num_workers=num_workers, pin_memory=pin_memory,\n        train_shuffle=train_shuffle)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:52:52.352275Z","iopub.execute_input":"2025-04-23T17:52:52.352887Z","iopub.status.idle":"2025-04-23T17:52:56.217427Z","shell.execute_reply.started":"2025-04-23T17:52:52.352850Z","shell.execute_reply":"2025-04-23T17:52:56.216667Z"}},"outputs":[{"name":"stdout","text":"Minimum sequence length: 77\nMaximum sequence length: 539\nTotal valid sequences: 55\nTrain dataset length: 8791\nVal dataset length: 2198\n/kaggle/working/convnet-edgemap\nTotal examples: 8447 (each is 8 frames → 1 target)\nTotal examples: 2094 (each is 8 frames → 1 target)\nlen of train loader: 528\nlen of val loader 131\nBatch input shape: torch.Size([16, 1, 8, 224, 224])\nBatch label shape: torch.Size([16, 1])\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class Conv2Plus1D(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding):\n        super().__init__()\n        kT, kH, kW = kernel_size\n        pT, pH, pW = padding\n\n        #spatial conv: only HxW\n        self.spatial = nn.Conv3d(in_channels, out_channels, kernel_size=(1, kH, kW), padding=(0, pH, pW), bias=False)\n        self.bn_spatial = nn.BatchNorm3d(out_channels)\n\n        #temporal conv: only T\n        self.temporal = nn.Conv3d(out_channels, out_channels, kernel_size=(kT, 1, 1), padding=(pT, 0, 0), bias=False)\n        self.bn_temporal = nn.BatchNorm3d(out_channels)\n\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.spatial(x)\n        x = self.bn_spatial(x)\n        x = self.relu(x)\n        x = self.temporal(x)\n        x = self.bn_temporal(x)\n        return self.relu(x)\n    \n\nclass ResidualMain(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super().__init__()\n        p = tuple(k//2 for k in kernel_size)\n        self.conv1 = Conv2Plus1D(in_channels, out_channels, kernel_size, p)\n        self.conv2 = Conv2Plus1D(out_channels, out_channels, kernel_size, p)\n\n    def forward(self, x):\n        return self.conv2(self.conv1(x))\n    \nclass Project(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.proj = nn.Sequential(\n            nn.Conv3d(in_channels, out_channels, kernel_size=1, bias=False),\n            nn.BatchNorm3d(out_channels)\n        )\n\n    def forward(self, x):\n        return self.proj(x)\n    \nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super().__init__()\n        self.main = ResidualMain(in_channels, out_channels, kernel_size)\n        self.need_proj = (in_channels != out_channels)\n        if self.need_proj:\n            self.proj = Project(in_channels, out_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        res = self.proj(x) if self.need_proj else x\n        out = self.main(x)\n        return self.relu(out + res)\n    \nclass ResizeVideo(nn.Module):\n    def __init__(self, out_size):\n        super().__init__()\n        self.out_size = out_size\n\n    def forward(self, x):\n        # x: [B, C, T, H, W]\n        B, C, T, H, W = x.shape\n        # collapse batch such that we treat frames as images\n        frames = x.permute(0,2,1,3,4).reshape(B*T, C, H, W)\n        frames = F.interpolate(frames, size=self.out_size,\n                               mode='bilinear', align_corners=False)\n        # reshape back\n        h, w = self.out_size\n        return frames.reshape(B, T, C, h, w).permute(0,2,1,3,4)\n    \nclass TemporalResNet(nn.Module):\n    def __init__(self, \n                 in_channels=1, \n                 seq_len=16,\n                 height=224,\n                 width=224):\n        super().__init__()\n\n        self.stem = nn.Sequential(\n            Conv2Plus1D(in_channels, 16, kernel_size=(3,7,7), padding=(1,3,3)),\n            nn.BatchNorm3d(16),\n            nn.ReLU(inplace=True),\n            ResizeVideo((height//2, width//2))\n        )\n        # 4 residual stages, downsampling spatially between them\n        self.stage1 = nn.Sequential(\n            ResidualBlock(16,  16, (3,3,3)),\n            ResizeVideo((height//4, width//4))\n        )\n        self.stage2 = nn.Sequential(\n            ResidualBlock(16,  32, (3,3,3)),\n            ResizeVideo((height//8, width//8))\n        )\n        self.stage3 = nn.Sequential(\n            ResidualBlock(32,  64, (3,3,3)),\n            ResizeVideo((height//16, width//16))\n        )\n        self.stage4 = nn.Sequential(\n            ResidualBlock(64,  128, (3,3,3)),\n            ResizeVideo((height//32, width//32))\n        )\n        self.stage5 = ResidualBlock(128, 256, (3,3,3))\n\n        # global spatio‑temporal pooling + linear regressor\n        self.pool = nn.AdaptiveAvgPool3d((1, 1, 1))  # [B, 128, 1,1,1]\n        self.fc1   = nn.Linear(256, 128)\n        self.fc2   = nn.Linear(128, 1)\n\n    def forward(self, x):\n        # x: [B, C, T, H, W]\n        x = self.stem(x)\n        x = self.stage1(x)\n        x = self.stage2(x)\n        x = self.stage3(x)\n        x = self.stage4(x)\n        x = self.stage5(x)\n        x = self.pool(x).view(x.size(0), -1) # [B, 128]\n        return self.fc2((self.fc1(x))) # [B,1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:54:41.970315Z","iopub.execute_input":"2025-04-23T17:54:41.971015Z","iopub.status.idle":"2025-04-23T17:54:41.986388Z","shell.execute_reply.started":"2025-04-23T17:54:41.970988Z","shell.execute_reply":"2025-04-23T17:54:41.985687Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def process_images(input_dir, output_dir):\n\n    os.makedirs(output_dir, exist_ok=True)\n    exts = ('.jpg', '.jpeg')\n\n    for fname in tqdm(os.listdir(input_dir)):\n        if not fname.lower().endswith(exts):\n            continue\n\n        img_path = os.path.join(input_dir, fname)\n        img = cv2.imread(img_path)\n        if img is None:\n            print(f\"Skipping {fname} (couldn’t read)\")\n            continue\n\n        height, width = img.shape[:2]\n\n        # grayscale + blur\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        blur = cv2.GaussianBlur(gray, (3, 3), 0)\n\n        # canny edges\n        edges = cv2.Canny(blur, 50, 70)\n\n        # build and apply ROI mask\n        mask = np.zeros_like(edges)\n        roi_corners = np.array([[\n            (0, height),\n            (width, height),\n            (width, int(height * 0.52)),\n            (0, int(height * 0.52))]], dtype=np.int32)\n        cv2.fillPoly(mask, roi_corners, 255)\n        roi_edges = cv2.bitwise_and(edges, mask)\n\n        y0 = int(height*0.52)\n        cropped_edges = roi_edges[y0:,:]\n\n        # save the masked edges with the same filename\n        out_path = os.path.join(output_dir, fname)\n        cv2.imwrite(out_path, cropped_edges)\n        # print(f\"Saved ROI edges to {out_path}\")\n\nif __name__ == \"__main__\":\n    input_folder  = \"/kaggle/input/sullychen/07012018/data\"\n    output_folder = \"/kaggle/working/edge_maps\"\n    process_images(input_folder, output_folder)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T00:24:20.227028Z","iopub.execute_input":"2025-04-23T00:24:20.227684Z","iopub.status.idle":"2025-04-23T00:42:03.328916Z","shell.execute_reply.started":"2025-04-23T00:24:20.227659Z","shell.execute_reply":"2025-04-23T00:42:03.328209Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 63825/63825 [17:43<00:00, 60.04it/s]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def plot_loss_accuracy(train_loss, val_loss, save_dir=None):\n    epochs = range(1, len(train_loss) + 1)\n\n    plt.figure(figsize=(12, 6))\n\n    # Plot Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_loss, label='Train Loss', color='blue', linestyle='-')\n    plt.plot(epochs, val_loss, label='Validation Loss', color='red', linestyle='--')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.title('Loss vs Epochs')\n    plt.tight_layout()\n    if save_dir:\n        plt.savefig(save_dir, bbox_inches='tight')\n        plt.close()\n    else:\n        plt.show()\n\ndef train_validate(train_loader, val_loader, optimizer, model, device, criterion, epochs=10, \n                   training_losses = None, val_losses = None, save_every=2, save_dir='/kaggle/working/convnet_sl_8_ss_4_filt_mse_1e-3_edges'):\n\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    \n    if training_losses is None: training_losses = []\n    if val_losses is None: val_losses = []\n    \n    for epoch in range(epochs): \n        model.train()\n        running_train_loss = 0.0\n        correct_train = 0\n        total_train   = 0\n\n        for (batch_x, batch_y) in tqdm(train_loader, desc=f'Training {epoch+1}/{epochs}:', ncols=100):\n\n            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n\n            optimizer.zero_grad()\n            predictions = model(batch_x)\n            loss = criterion(predictions, batch_y)\n            loss.backward()\n            optimizer.step()\n\n            running_train_loss += loss.item()\n            _, pred_labels = predictions.max(dim=1)\n            correct_train += (pred_labels == batch_y).sum().item()\n            total_train += batch_y.size(0)\n\n        avg_train_loss = running_train_loss / len(train_loader)\n\n        training_losses.append(avg_train_loss)\n        print(f\"Train Loss: {avg_train_loss:.4f}\")\n\n        #validation loop\n        model.eval()\n        running_val_loss = 0.0\n        correct_val = 0\n        total_val   = 0\n        for (batch_x, batch_y) in tqdm(val_loader, desc=f'Val {epoch+1}/{epochs}:', ncols=100):\n            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n\n            with torch.no_grad():\n                predictions = model(batch_x)\n                loss = criterion(predictions, batch_y)\n\n            running_val_loss += loss.item()\n            _, pred_labels = predictions.max(dim=1)\n            correct_val += (pred_labels == batch_y).sum().item()\n            total_val += batch_y.size(0)\n\n        avg_val_loss = running_val_loss / len(val_loader)\n\n        val_losses.append(avg_val_loss)\n        print(f\"Val Loss: {avg_val_loss:.4f}\")\n\n        if (epoch+1) % save_every == 0:\n\n            checkpoint = {\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'epoch': epoch + 1,\n            'training_losses': training_losses,\n            'val_losses': val_losses,\n            }\n\n            model_path = os.path.join(save_dir, f'model_epoch{epoch+1}.pth')\n            torch.save(checkpoint, model_path)\n            print(f\"Checkpoint saved to {save_dir}\\n\")\n\n    return training_losses, val_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:53:34.616483Z","iopub.execute_input":"2025-04-23T17:53:34.617336Z","iopub.status.idle":"2025-04-23T17:53:34.628394Z","shell.execute_reply.started":"2025-04-23T17:53:34.617304Z","shell.execute_reply":"2025-04-23T17:53:34.627523Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"train_dataset_path = '/kaggle/working/convnet-edgemap/train_flt_ncp_tt_0.08_bb_32_ba_32.csv'\nval_dataset_path = '/kaggle/working/convnet-edgemap/val_flt_ncp_tt_0.08_bb_32_ba_32.csv'\nseq_len = 8\nimgh = 123\nimgw = 455\nstep_size = 1\ncrop = False\n\nbatch_size = 32\nnum_workers = 4\nprefetch_factor = 2\npin_memory = True\ntrain_shuffle = False\n\n\n\ntrain_dataset , val_dataset = create_train_val_dataset(train_csv_file = train_dataset_path,\n                                                     val_csv_file = val_dataset_path,\n                                                     seq_len=seq_len, imgh=imgh, imgw=imgw,\n                                                     step_size=step_size, crop=crop,\n                                                     mean=[0.5], std=[0.5])\n\n#gets dataloaders from dataset\ntrain_loader, val_loader = create_train_val_loader(train_dataset, val_dataset,\n                                                batch_size=batch_size, \n                                                num_workers=num_workers, \n                                                prefetch_factor=prefetch_factor,\n                                                pin_memory=pin_memory, \n                                                train_shuffle=train_shuffle)\n\nmodel = TemporalResNet(in_channels=1, height=123, width=455).to(device=torch.device('cuda'))\nprint(summary(model, input_size=(32, 1, 8, 455, 123)))\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\ntrain_losses, val_losses = train_validate(train_loader, val_loader, optimizer, model, \n                                          torch.device('cuda'), criterion, 20,\n                                          save_dir='/kaggle/working/convnet_sl_8_ss_4_filt_mse_1e-3_edges_5stages')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T17:55:37.369975Z","iopub.execute_input":"2025-04-23T17:55:37.370531Z"}},"outputs":[{"name":"stdout","text":"Total examples: 8447 (each is 8 frames → 1 target)\nTotal examples: 2094 (each is 8 frames → 1 target)\nlen of train loader: 264\nlen of val loader 66\nBatch input shape: torch.Size([32, 1, 8, 455, 123])\nBatch label shape: torch.Size([32, 1])\n===============================================================================================\nLayer (type:depth-idx)                        Output Shape              Param #\n===============================================================================================\nTemporalResNet                                [32, 1]                   --\n├─Sequential: 1-1                             [32, 16, 8, 61, 227]      --\n│    └─Conv2Plus1D: 2-1                       [32, 16, 8, 455, 123]     --\n│    │    └─Conv3d: 3-1                       [32, 16, 8, 455, 123]     784\n│    │    └─BatchNorm3d: 3-2                  [32, 16, 8, 455, 123]     32\n│    │    └─ReLU: 3-3                         [32, 16, 8, 455, 123]     --\n│    │    └─Conv3d: 3-4                       [32, 16, 8, 455, 123]     768\n│    │    └─BatchNorm3d: 3-5                  [32, 16, 8, 455, 123]     32\n│    │    └─ReLU: 3-6                         [32, 16, 8, 455, 123]     --\n│    └─BatchNorm3d: 2-2                       [32, 16, 8, 455, 123]     32\n│    └─ReLU: 2-3                              [32, 16, 8, 455, 123]     --\n│    └─ResizeVideo: 2-4                       [32, 16, 8, 61, 227]      --\n├─Sequential: 1-2                             [32, 16, 8, 30, 113]      --\n│    └─ResidualBlock: 2-5                     [32, 16, 8, 61, 227]      --\n│    │    └─ResidualMain: 3-7                 [32, 16, 8, 61, 227]      6,272\n│    │    └─ReLU: 3-8                         [32, 16, 8, 61, 227]      --\n│    └─ResizeVideo: 2-6                       [32, 16, 8, 30, 113]      --\n├─Sequential: 1-3                             [32, 32, 8, 15, 56]       --\n│    └─ResidualBlock: 2-7                     [32, 32, 8, 30, 113]      --\n│    │    └─Project: 3-9                      [32, 32, 8, 30, 113]      576\n│    │    └─ResidualMain: 3-10                [32, 32, 8, 30, 113]      20,224\n│    │    └─ReLU: 3-11                        [32, 32, 8, 30, 113]      --\n│    └─ResizeVideo: 2-8                       [32, 32, 8, 15, 56]       --\n├─Sequential: 1-4                             [32, 64, 8, 7, 28]        --\n│    └─ResidualBlock: 2-9                     [32, 64, 8, 15, 56]       --\n│    │    └─Project: 3-12                     [32, 64, 8, 15, 56]       2,176\n│    │    └─ResidualMain: 3-13                [32, 64, 8, 15, 56]       80,384\n│    │    └─ReLU: 3-14                        [32, 64, 8, 15, 56]       --\n│    └─ResizeVideo: 2-10                      [32, 64, 8, 7, 28]        --\n├─Sequential: 1-5                             [32, 128, 8, 3, 14]       --\n│    └─ResidualBlock: 2-11                    [32, 128, 8, 7, 28]       --\n│    │    └─Project: 3-15                     [32, 128, 8, 7, 28]       8,448\n│    │    └─ResidualMain: 3-16                [32, 128, 8, 7, 28]       320,512\n│    │    └─ReLU: 3-17                        [32, 128, 8, 7, 28]       --\n│    └─ResizeVideo: 2-12                      [32, 128, 8, 3, 14]       --\n├─ResidualBlock: 1-6                          [32, 256, 8, 3, 14]       --\n│    └─Project: 2-13                          [32, 256, 8, 3, 14]       --\n│    │    └─Sequential: 3-18                  [32, 256, 8, 3, 14]       33,280\n│    └─ResidualMain: 2-14                     [32, 256, 8, 3, 14]       --\n│    │    └─Conv2Plus1D: 3-19                 [32, 256, 8, 3, 14]       492,544\n│    │    └─Conv2Plus1D: 3-20                 [32, 256, 8, 3, 14]       787,456\n│    └─ReLU: 2-15                             [32, 256, 8, 3, 14]       --\n├─AdaptiveAvgPool3d: 1-7                      [32, 256, 1, 1, 1]        --\n├─Linear: 1-8                                 [32, 128]                 32,896\n├─Linear: 1-9                                 [32, 1]                   129\n===============================================================================================\nTotal params: 1,786,545\nTrainable params: 1,786,545\nNon-trainable params: 0\nTotal mult-adds (Units.GIGABYTES): 109.94\n===============================================================================================\nInput size (MB): 57.31\nForward/backward pass size (MB): 16855.92\nParams size (MB): 7.15\nEstimated Total Size (MB): 16920.38\n===============================================================================================\n","output_type":"stream"},{"name":"stderr","text":"Training 1/20:: 100%|█████████████████████████████████████████████| 264/264 [03:20<00:00,  1.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0483\n","output_type":"stream"},{"name":"stderr","text":"Val 1/20:: 100%|████████████████████████████████████████████████████| 66/66 [00:14<00:00,  4.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0338\n","output_type":"stream"},{"name":"stderr","text":"Training 2/20:: 100%|█████████████████████████████████████████████| 264/264 [03:20<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0377\n","output_type":"stream"},{"name":"stderr","text":"Val 2/20:: 100%|████████████████████████████████████████████████████| 66/66 [00:13<00:00,  4.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0244\nCheckpoint saved to /kaggle/working/convnet_sl_8_ss_4_filt_mse_1e-3_edges_5stages\n\n","output_type":"stream"},{"name":"stderr","text":"Training 3/20:: 100%|█████████████████████████████████████████████| 264/264 [03:20<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0274\n","output_type":"stream"},{"name":"stderr","text":"Val 3/20:: 100%|████████████████████████████████████████████████████| 66/66 [00:13<00:00,  5.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0273\n","output_type":"stream"},{"name":"stderr","text":"Training 4/20:: 100%|█████████████████████████████████████████████| 264/264 [03:20<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0238\n","output_type":"stream"},{"name":"stderr","text":"Val 4/20:: 100%|████████████████████████████████████████████████████| 66/66 [00:13<00:00,  4.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0300\nCheckpoint saved to /kaggle/working/convnet_sl_8_ss_4_filt_mse_1e-3_edges_5stages\n\n","output_type":"stream"},{"name":"stderr","text":"Training 5/20:: 100%|█████████████████████████████████████████████| 264/264 [03:20<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0217\n","output_type":"stream"},{"name":"stderr","text":"Val 5/20:: 100%|████████████████████████████████████████████████████| 66/66 [00:13<00:00,  4.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0279\n","output_type":"stream"},{"name":"stderr","text":"Training 6/20:: 100%|█████████████████████████████████████████████| 264/264 [03:20<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0187\n","output_type":"stream"},{"name":"stderr","text":"Val 6/20:: 100%|████████████████████████████████████████████████████| 66/66 [00:13<00:00,  4.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0281\nCheckpoint saved to /kaggle/working/convnet_sl_8_ss_4_filt_mse_1e-3_edges_5stages\n\n","output_type":"stream"},{"name":"stderr","text":"Training 7/20:: 100%|█████████████████████████████████████████████| 264/264 [03:20<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0162\n","output_type":"stream"},{"name":"stderr","text":"Val 7/20:: 100%|████████████████████████████████████████████████████| 66/66 [00:13<00:00,  4.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0265\n","output_type":"stream"},{"name":"stderr","text":"Training 8/20:: 100%|█████████████████████████████████████████████| 264/264 [03:20<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0129\n","output_type":"stream"},{"name":"stderr","text":"Val 8/20:: 100%|████████████████████████████████████████████████████| 66/66 [00:13<00:00,  4.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0268\nCheckpoint saved to /kaggle/working/convnet_sl_8_ss_4_filt_mse_1e-3_edges_5stages\n\n","output_type":"stream"},{"name":"stderr","text":"Training 9/20:: 100%|█████████████████████████████████████████████| 264/264 [03:20<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0099\n","output_type":"stream"},{"name":"stderr","text":"Val 9/20:: 100%|████████████████████████████████████████████████████| 66/66 [00:13<00:00,  4.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0246\n","output_type":"stream"},{"name":"stderr","text":"Training 10/20:: 100%|████████████████████████████████████████████| 264/264 [03:20<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0075\n","output_type":"stream"},{"name":"stderr","text":"Val 10/20:: 100%|███████████████████████████████████████████████████| 66/66 [00:14<00:00,  4.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0305\nCheckpoint saved to /kaggle/working/convnet_sl_8_ss_4_filt_mse_1e-3_edges_5stages\n\n","output_type":"stream"},{"name":"stderr","text":"Training 11/20:: 100%|████████████████████████████████████████████| 264/264 [03:20<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0072\n","output_type":"stream"},{"name":"stderr","text":"Val 11/20:: 100%|███████████████████████████████████████████████████| 66/66 [00:13<00:00,  4.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0291\n","output_type":"stream"},{"name":"stderr","text":"Training 12/20:: 100%|████████████████████████████████████████████| 264/264 [03:20<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0081\n","output_type":"stream"},{"name":"stderr","text":"Val 12/20:: 100%|███████████████████████████████████████████████████| 66/66 [00:13<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0295\nCheckpoint saved to /kaggle/working/convnet_sl_8_ss_4_filt_mse_1e-3_edges_5stages\n\n","output_type":"stream"},{"name":"stderr","text":"Training 13/20:: 100%|████████████████████████████████████████████| 264/264 [03:20<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0069\n","output_type":"stream"},{"name":"stderr","text":"Val 13/20:: 100%|███████████████████████████████████████████████████| 66/66 [00:13<00:00,  4.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0235\n","output_type":"stream"},{"name":"stderr","text":"Training 14/20:: 100%|████████████████████████████████████████████| 264/264 [03:20<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0065\n","output_type":"stream"},{"name":"stderr","text":"Val 14/20:: 100%|███████████████████████████████████████████████████| 66/66 [00:13<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0248\nCheckpoint saved to /kaggle/working/convnet_sl_8_ss_4_filt_mse_1e-3_edges_5stages\n\n","output_type":"stream"},{"name":"stderr","text":"Training 15/20:: 100%|████████████████████████████████████████████| 264/264 [03:20<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0074\n","output_type":"stream"},{"name":"stderr","text":"Val 15/20:: 100%|███████████████████████████████████████████████████| 66/66 [00:13<00:00,  5.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0256\n","output_type":"stream"},{"name":"stderr","text":"Training 16/20:: 100%|████████████████████████████████████████████| 264/264 [03:20<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0086\n","output_type":"stream"},{"name":"stderr","text":"Val 16/20:: 100%|███████████████████████████████████████████████████| 66/66 [00:15<00:00,  4.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0269\nCheckpoint saved to /kaggle/working/convnet_sl_8_ss_4_filt_mse_1e-3_edges_5stages\n\n","output_type":"stream"},{"name":"stderr","text":"Training 17/20:: 100%|████████████████████████████████████████████| 264/264 [03:20<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0088\n","output_type":"stream"},{"name":"stderr","text":"Val 17/20:: 100%|███████████████████████████████████████████████████| 66/66 [00:13<00:00,  5.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0260\n","output_type":"stream"},{"name":"stderr","text":"Training 18/20:: 100%|████████████████████████████████████████████| 264/264 [03:20<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0070\n","output_type":"stream"},{"name":"stderr","text":"Val 18/20:: 100%|███████████████████████████████████████████████████| 66/66 [00:13<00:00,  4.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.0272\nCheckpoint saved to /kaggle/working/convnet_sl_8_ss_4_filt_mse_1e-3_edges_5stages\n\n","output_type":"stream"},{"name":"stderr","text":"Training 19/20:: 100%|████████████████████████████████████████████| 264/264 [03:20<00:00,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0063\n","output_type":"stream"},{"name":"stderr","text":"Val 19/20::  55%|███████████████████████████▊                       | 36/66 [00:07<00:05,  5.31it/s]","output_type":"stream"}],"execution_count":null}]}