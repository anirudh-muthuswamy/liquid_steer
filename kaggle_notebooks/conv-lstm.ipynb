{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11382018,"sourceType":"datasetVersion","datasetId":7126841},{"sourceId":352265,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":293906,"modelId":314532}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch\nimport pandas as pd\nimport re\nimport cv2\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm\nfrom collections import OrderedDict\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:22:34.312021Z","iopub.execute_input":"2025-04-22T20:22:34.312398Z","iopub.status.idle":"2025-04-22T20:22:34.318887Z","shell.execute_reply.started":"2025-04-22T20:22:34.312370Z","shell.execute_reply":"2025-04-22T20:22:34.318113Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class ConvLSTMCell(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n        super().__init__()\n        self.input_dim  = input_dim\n    \n        self.hidden_dim = hidden_dim\n        self.kernel_size = kernel_size\n        self.padding = (kernel_size[0] // 2, kernel_size[1] // 2)\n        # output 4*hidden for the gates\n        self.conv = nn.Conv2d(\n            in_channels=input_dim + hidden_dim,\n            out_channels=4 * hidden_dim,\n            kernel_size=kernel_size,\n            padding=self.padding,\n            bias=bias\n        )\n\n    def forward(self, x, hc):\n        h_cur, c_cur = hc\n        # concatenate on channel axis\n        combined = torch.cat([x, h_cur], dim=1)\n        conv_out = self.conv(combined)\n        cc_i, cc_f, cc_o, cc_g = torch.split(conv_out, self.hidden_dim, dim=1)\n        i = torch.sigmoid(cc_i)\n        f = torch.sigmoid(cc_f)\n        o = torch.sigmoid(cc_o)\n        g = torch.tanh(cc_g)\n\n        c_next = f * c_cur + i * g\n        h_next = o * torch.tanh(c_next)\n        return h_next, c_next\n\n    def init_hidden(self, batch_size, image_size):\n        height, width = image_size\n        device = next(self.parameters()).device\n        return (\n            torch.zeros(batch_size, self.hidden_dim, height, width, device=device),\n            torch.zeros(batch_size, self.hidden_dim, height, width, device=device),\n        )\n\n\nclass STConvLSTM(nn.Module):\n    def __init__(\n        self,\n        seq_len=3,\n        height=66,\n        width=200,\n        input_channels=3,\n        hidden_channels=8,\n        fc_units=50,\n        dropout=0.5\n    ):\n        super().__init__()\n        self.seq_len = seq_len\n        self.height = height\n        self.width  = width\n\n        # stack of 4 ConvLSTM layers\n        self.cells = nn.ModuleList([\n            ConvLSTMCell(input_channels, hidden_channels, (3, 3)),\n            ConvLSTMCell(hidden_channels, hidden_channels, (3, 3)),\n            ConvLSTMCell(hidden_channels, hidden_channels, (3, 3)),\n            ConvLSTMCell(hidden_channels, hidden_channels, (3, 3)),\n        ])\n        # one BatchNorm3d per layer (treating time as depth)\n        self.bns = nn.ModuleList([\n            nn.BatchNorm3d(hidden_channels) for _ in range(4)\n        ])\n\n        self.conv3d = nn.Conv3d(\n            in_channels=hidden_channels,\n            out_channels=2,\n            kernel_size=(3, 3, 3),\n            padding=1\n        )\n        self.pool3d = nn.MaxPool3d((2, 2, 2))\n\n        #flattened size: out_channels * (seq_len//2) * (height//2) * (width//2)\n        flat_size = 2 * (seq_len // 2) * (height // 2) * (width // 2)\n        self.fc1     = nn.Linear(flat_size, fc_units)\n        self.leaky   = nn.LeakyReLU(0.2)\n        self.dropout = nn.Dropout(dropout)\n        self.fc2     = nn.Linear(fc_units, 1)\n\n    def _run_convlstm(self, cell: ConvLSTMCell, bn: nn.BatchNorm3d, x: torch.Tensor) -> torch.Tensor:\n\n        #x: (batch, seq, channels, H, W)\n        b, seq, c, h, w = x.size()\n        h_t, c_t = cell.init_hidden(b, (h, w))\n        outputs = []\n        for t in range(seq):\n            h_t, c_t = cell(x[:, t], (h_t, c_t))\n            outputs.append(h_t)\n        x_seq = torch.stack(outputs, dim=1)       #(b, seq, hidden, H, W)\n        x_bn  = x_seq.permute(0, 2, 1, 3, 4)      #(b, hidden, seq, H, W)\n        x_bn  = bn(x_bn)\n        return x_bn.permute(0, 2, 1, 3, 4)        #(b, seq, hidden, H, W)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # (batch, seq, H, W, C) \n        # x = x / 255.0\n        # (batch, seq, C, H, W)\n\n        for cell, bn in zip(self.cells, self.bns):\n            x = self._run_convlstm(cell, bn, x)\n\n        x = x.permute(0, 2, 1, 3, 4)\n        x = self.conv3d(x)\n        x = self.pool3d(x)\n        \n        x = x.reshape(x.size(0), -1)\n        x = self.fc1(x)\n        x = self.leaky(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n\n# example usage\nif __name__ == \"__main__\":\n    model = STConvLSTM(seq_len=6, height=224, width=224)\n    sample = torch.randn(16, 6, 3, 224, 224)   # batch=4, seq=3, H=66, W=200, C=3\n    out = model(sample)\n    print(out.shape)  # â†’ torch.Size([4, 1])\n\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:22:34.885328Z","iopub.execute_input":"2025-04-22T20:22:34.885609Z","iopub.status.idle":"2025-04-22T20:22:45.565313Z","shell.execute_reply.started":"2025-04-22T20:22:34.885588Z","shell.execute_reply":"2025-04-22T20:22:45.564487Z"}},"outputs":[{"name":"stdout","text":"torch.Size([16, 1])\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"pd.set_option(\"display.max_rows\", 200)\nplt.ion() \n\ndef get_full_image_filepaths(data_dir):\n    filepaths = os.listdir(data_dir)\n    sorted_filepaths = sorted(filepaths, key=lambda x: int(re.search(r'\\d+', x).group()))\n    full_filepaths = [os.path.join(data_dir, file) for file in sorted_filepaths]\n\n    return full_filepaths\n\ndef get_steering_angles(path):\n    file = open(path, 'r')\n    lines = file.readlines()\n    #change this to line[:-1] if using kaggle dataset else keep the same for sullychen dataset\n    steering_angles = [(line[:-1]).split(' ')[1].split(',')[0] for line in lines]\n    timestamps = [line.strip().split(' ', 1)[1].split(',')[1] for line in lines]\n\n    return [steering_angles, timestamps]\n\ndef convert_to_df(full_filepaths, steering_angles, timestamps, norm=True):\n    data = pd.DataFrame({'filepath':full_filepaths,'steering_angle':steering_angles, 'timestamps':timestamps})\n    data['parsed_timestamp'] = data['timestamps'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S:%f'))\n    data = data.drop('timestamps', axis=1)\n    data = data.reset_index(drop=True)\n    data['steering_angle'] = data['steering_angle'].astype('float')\n    #normalize steering angles to -1,1 based on actual steering range (-360, 360) of wheel\n    if norm == True:\n        data['steering_angle'] = data['steering_angle'] / 360\n    return data\n\ndef display_images_with_angles(data_pd, steering_wheel_img):\n\n    # steering wheel image\n    img = cv2.imread(steering_wheel_img,0)\n    rows,cols = img.shape\n    i = 0\n    xs = data_pd['filepath'].values\n    ys = data_pd['steering_angle'].values\n\n    #Press q to stop \n    while(cv2.waitKey(100) != ord('q')) and i < len(xs):\n        try:\n            # Driving Test Image displayed as Video\n            full_image = cv2.imread(xs[i])\n\n            degrees = ys[i] * 360\n            print(\"Steering angle: \" + str(degrees) + \" (actual)\")\n            cv2.imshow(\"frame\", full_image)\n\n            # Angle at which the steering wheel image should be rotated\n            M = cv2.getRotationMatrix2D((cols/2,rows/2),-degrees,1)\n            dst = cv2.warpAffine(img,M,(cols,rows))\n\n            cv2.imshow(\"steering wheel\", dst)\n        except:\n            print('ERROR at', i)\n        i += 1\n    cv2.destroyAllWindows()\n\ndef disp_freq_steering_angles(data_pd):\n    # Define bin edges from -1 to 1 with step 0.1\n    bin_edges = list(range(-10, 11))  # Since steering angle is between -1 and 1, multiply by 10\n    bin_edges = [x / 10 for x in bin_edges]  # Convert back to decimal values\n\n    # Assign steering angles to bins\n    data_pd['binned'] = pd.cut(data_pd['steering_angle'].astype('float'), bins=bin_edges, right=False)\n\n    # Count occurrences in each bin\n    bin_counts = data_pd['binned'].value_counts().sort_index()\n\n    # Plot bar chart\n    plt.figure(figsize=(12, 5))\n    bin_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n\n    # Formatting\n    plt.xlabel(\"Steering Angle Range\")\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Frequency of Steering Angles in 0.1 Intervals\")\n    plt.xticks(rotation=45)\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n\n    plt.show()\n\ndef disp_start_and_end_in_filtered_data(data_pd_filtered):\n    turn_starts = data_pd_filtered[data_pd_filtered['turn_shift'] == 1]\n    turn_ends = data_pd_filtered[data_pd_filtered['turn_shift'] == -1]\n\n    plt.figure(figsize=(12, 5))\n    plt.plot(data_pd_filtered.index, data_pd_filtered['steering_angle'], label=\"Steering Angle\", alpha=0.5)\n    plt.scatter(turn_starts.index, turn_starts['steering_angle'], color='red', label=\"Turn Start\", marker=\"o\")\n    plt.scatter(turn_ends.index, turn_ends['steering_angle'], color='blue', label=\"Turn End\", marker=\"x\")\n\n    plt.xlabel(\"Frame Index\")\n    plt.ylabel(\"Steering Angle\")\n    plt.title(\"Turn Start and End Points in Steering Angle Data\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef filter_df_on_turns(data_pd, turn_threshold = 0.06, buffer_before = 60, buffer_after = 60):\n    # Parameters\n    turn_threshold = turn_threshold  # Define turn threshold (absolute value)\n    buffer_before = buffer_before    # Frames to include before a turn\n    buffer_after = buffer_after     # Frames to include after a turn\n\n    # Load your dataset (assuming it's a DataFrame named df)\n    data_pd['index'] = data_pd.index  # Preserve original ordering if needed\n\n    # Identify where turning happens\n    data_pd['turning'] = (data_pd['steering_angle'].abs() > turn_threshold).astype(int)\n\n    # Find where turns start and end\n    data_pd['turn_shift'] = data_pd['turning'].diff()  # 1 indicates start, -1 indicates end\n\n    # Get turn start and end indices\n    turn_starts = data_pd[data_pd['turn_shift'] == 1].index\n    turn_ends = data_pd[data_pd['turn_shift'] == -1].index\n\n    # Ensure equal number of start and end points\n    if len(turn_ends) > 0 and turn_starts[0] > turn_ends[0]:  \n        turn_ends = turn_ends[1:]  # Drop the first turn_end if it comes before a start\n\n    # Selected indices for keeping\n    selected_indices = set()\n\n    for start, end in zip(turn_starts, turn_ends):\n        # Include a buffer of frames before and after\n        start_idx = max(0, start - buffer_before)\n        end_idx = min(len(data_pd) - 1, end + buffer_after)\n        \n        # Add indices to selection\n        selected_indices.update(range(start_idx, end_idx + 1))\n\n    # Create filtered dataset\n    data_pd_filtered = data_pd.loc[sorted(selected_indices)].reset_index(drop=True)\n\n    # Drop temporary columns\n    data_pd_filtered = data_pd_filtered.drop(columns=['turning', 'turn_shift'])\n\n    # Detect sequence breaks (where the original index is not continuous)\n    data_pd_filtered[\"sequence_id\"] = (data_pd_filtered[\"index\"].diff() != 1).cumsum()\n\n    return data_pd_filtered\n\ndef group_data_by_sequences(data_pd_filtered):\n    sequence_lengths = data_pd_filtered.groupby(\"sequence_id\").size()\n\n    print(f\"Minimum sequence length: {min(sequence_lengths)}\")\n    print(f\"Maximum sequence length: {max(sequence_lengths)}\")\n\n    # plt.plot(sequence_lengths)\n\n    # Keep only sequences with at least 10 frames (adjust as needed)\n    valid_sequences = sequence_lengths[sequence_lengths >= 40].index\n    data_pd_filtered = data_pd_filtered[data_pd_filtered[\"sequence_id\"].isin(valid_sequences)]\n\n    print(f\"Total valid sequences: {len(valid_sequences)}\")\n\n    return data_pd_filtered\n\ndef get_preprocessed_data_pd(data_dir, steering_angles_txt_path, filter = True,\n                             turn_threshold = 0.06, buffer_before = 60, buffer_after = 60,\n                             norm=True, save_dir = 'data/csv_files'):\n    img_paths = get_full_image_filepaths(data_dir)\n    steering_angles, timestamps = get_steering_angles(steering_angles_txt_path)\n\n    data_pd = convert_to_df(img_paths, steering_angles, timestamps, norm)\n    if filter and norm:\n        data_pd_filtered = filter_df_on_turns(data_pd, turn_threshold = turn_threshold, \n                                            buffer_before = buffer_before, buffer_after = buffer_after)\n        data_pd_filtered = group_data_by_sequences(data_pd_filtered)\n\n        # Save\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        data_pd_filtered.to_csv(os.path.join(save_dir,f\"flt_ncp_tt_{turn_threshold}_bb_{buffer_before}_ba_{buffer_after}.csv\"), index=False)\n\n        return data_pd_filtered\n    \n    elif filter and not norm:\n        print('Error: If filtering, then steering values should be normalized (-1, 1), set norm to True')\n        exit(1)\n\n    else:\n        sequence_id = 0\n        sequence_ids = [sequence_id]\n\n        # Iterate through rows to calculate time differences (if greater than 3 seconds \n        # or not) and assign sequence IDs\n        for i in range(1, len(data_pd)):\n            time_diff = (data_pd['parsed_timestamp'][i] - data_pd['parsed_timestamp'][i-1]).total_seconds()\n            if time_diff > 3:\n                sequence_id += 1\n            sequence_ids.append(sequence_id)\n\n        # Add sequence_id column to DataFrame\n        data_pd['sequence_id'] = sequence_ids\n\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n        data_pd.to_csv(os.path.join(save_dir,f\"ncp_unfiltered.csv\"), index=False)\n        \n        return data_pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:22:45.589915Z","iopub.execute_input":"2025-04-22T20:22:45.590148Z","iopub.status.idle":"2025-04-22T20:22:45.613305Z","shell.execute_reply.started":"2025-04-22T20:22:45.590131Z","shell.execute_reply":"2025-04-22T20:22:45.612475Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def df_split_train_val(df_filtered, train_csv_filename, val_csv_filename,\n                       save_dir='data/csv_files',train_size = 0.8):\n    train_dataset = df_filtered[:int(train_size * len(df_filtered))]\n    val_dataset = df_filtered[int(train_size * len(df_filtered)):]\n    print('Train dataset length:', len(train_dataset))\n    print('Val dataset length:', len(val_dataset))\n    print(save_dir)\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    train_dataset.to_csv(os.path.join(save_dir,train_csv_filename), index=False)\n    val_dataset.to_csv(os.path.join(save_dir,val_csv_filename), index=False)\n\n    return os.path.join(save_dir,train_csv_filename), os.path.join(save_dir,val_csv_filename)\n\ndef calculate_mean_and_std(dataset_path):\n    num_pixels = 0\n    channel_sum = np.zeros(3)  # Assuming RGB images, change to (1,) for grayscale\n    channel_sum_squared = np.zeros(3)  # Assuming RGB images, change to (1,) for grayscale\n\n    for root, _, files in os.walk(dataset_path):\n        for file in files:\n            image_path = os.path.join(root, file)\n            image = Image.open(image_path).convert('RGB')  # Convert to RGB if needed\n\n            pixels = np.array(image) / 255.0  # Normalize pixel values between 0 and 1\n            num_pixels += pixels.size // 3  # Assuming RGB images, change to 1 for grayscale\n\n            channel_sum += np.sum(pixels, axis=(0, 1))\n            channel_sum_squared += np.sum(pixels ** 2, axis=(0, 1))\n\n    mean = channel_sum / num_pixels\n    std = np.sqrt((channel_sum_squared / num_pixels) - mean ** 2)\n    return mean, std\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:22:45.614828Z","iopub.execute_input":"2025-04-22T20:22:45.615093Z","iopub.status.idle":"2025-04-22T20:22:45.633990Z","shell.execute_reply.started":"2025-04-22T20:22:45.615071Z","shell.execute_reply":"2025-04-22T20:22:45.633275Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import math\nclass CustomDataset(Dataset):\n    def __init__(self,\n                 csv_file,\n                 seq_len,\n                 imgh=224,\n                 imgw=224,\n                 step_size=1,\n                 crop=True,\n                 transform=None):\n        \"\"\"\n        Now builds only those windows of length seq_len\n        for which there *is* a next frame to predict.\n        \"\"\"\n        self.df        = pd.read_csv(csv_file)\n        self.seq_len   = seq_len\n        self.imgh      = imgh\n        self.imgw      = imgw\n        self.step_size = step_size\n        self.crop      = crop\n        self.transform = transform\n\n        self.sequences = []  # will hold tuples (seq_df, next_angle)\n\n        # group once by sequence_id\n        for seq_id, seq_data in self.df.groupby(\"sequence_id\"):\n            # we need at least seq_len + 1 frames to form one training example\n            N = len(seq_data)\n            for start in range(0, N - seq_len, step_size):\n                window = seq_data.iloc[start : start + seq_len]\n                next_angle = seq_data.iloc[start + seq_len][\"steering_angle\"]\n                self.sequences.append((window.reset_index(drop=True), next_angle))\n\n        print(f\"Total examples: {len(self.sequences)} \"\n              f\"(each is {seq_len} frames â†’ 1 target)\")\n\n    def _crop_lower_half(self, img, keep_ratio=0.6):\n        h = img.shape[0]\n        return img[int(h*(1-keep_ratio)) :, :, :]\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        seq_df, next_angle = self.sequences[idx]\n\n        # 1) load & preprocess the seq_len frames\n        imgs = []\n        for fp in seq_df[\"filepath\"]:\n            img = cv2.cvtColor(cv2.imread(fp), cv2.COLOR_BGR2RGB)\n            if self.crop:\n                img = self._crop_lower_half(img)\n            img = cv2.resize(img, (self.imgh, self.imgw))\n            if self.transform:\n                img = self.transform(img)  # â†’ CÃ—HÃ—W\n            imgs.append(img)\n\n        # shape (seq_len, C, H, W)\n        x = torch.stack(imgs, dim=0)\n        y = torch.tensor([next_angle], dtype=torch.float32)\n        return x, y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:22:45.634694Z","iopub.execute_input":"2025-04-22T20:22:45.634942Z","iopub.status.idle":"2025-04-22T20:22:45.647148Z","shell.execute_reply.started":"2025-04-22T20:22:45.634921Z","shell.execute_reply":"2025-04-22T20:22:45.646455Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def create_train_val_dataset(train_csv_file, \n                              val_csv_file,\n                              seq_len = 32, \n                              imgw = 224,\n                              imgh = 224,\n                              step_size = 32,\n                              crop = True,\n                              mean=[0.485, 0.456, 0.406],\n                              std=[0.229, 0.224, 0.225]):\n    \n    transform = transforms.Compose([\n        transforms.ToTensor(),  # Convert image to PyTorch tensor\n        transforms.Normalize(mean=mean, std=std)\n    ])\n\n    train_dataset = CustomDataset(csv_file=train_csv_file, seq_len=seq_len, imgh = imgh, imgw=imgw,\n                                  step_size=step_size,crop=crop, transform=transform)\n    val_dataset = CustomDataset(csv_file=val_csv_file, seq_len=seq_len,imgh = imgh, imgw=imgw,\n                                step_size=step_size,crop=crop, transform=transform)\n\n    return train_dataset, val_dataset\n\ndef create_train_val_loader(train_dataset, val_dataset, train_sampler=None, val_sampler=None, batch_size=8,\n                            num_workers=4, prefetch_factor=2, pin_memory=True, train_shuffle=False):\n\n    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size, num_workers=num_workers, \n                              prefetch_factor=prefetch_factor,pin_memory=pin_memory, shuffle=train_shuffle)\n    val_loader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size, num_workers=num_workers, \n                            prefetch_factor=prefetch_factor, pin_memory=pin_memory, shuffle=False)\n\n    print('len of train loader:', len(train_loader))\n    print('len of val loader', len(val_loader))\n\n    for (inputs, labels) in train_loader:\n        print(\"Batch input shape:\", inputs.shape)\n        print(\"Batch label shape:\", labels.shape)\n        break\n\n    return train_loader, val_loader\n\ndef get_loaders_for_training(data_dir, steering_angles_path, step_size, seq_len, imgh, imgw, filter, turn_threshold, \n                                     buffer_before, buffer_after, crop=True, train_size=0.8, save_dir='data/csv_files', \n                                     norm=True, batch_size=16, num_workers=4, prefetch_factor=4, pin_memory=True, train_shuffle=False):\n    \n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    \n    #preprocesses data\n    data_preprocessed_pd = get_preprocessed_data_pd(data_dir, steering_angles_path, filter, turn_threshold, \n                                     buffer_before, buffer_after, norm, save_dir)\n\n    print('max:', max(data_preprocessed_pd['steering_angle']), 'rad:', max(data_preprocessed_pd['steering_angle'] * math.pi / 180.0))\n    print('min:', min(data_preprocessed_pd['steering_angle']), 'rad:', min(data_preprocessed_pd['steering_angle'] * math.pi / 180.0))\n\n    if filter:\n        train_csv_filename = f\"train_flt_ncp_tt_{turn_threshold}_bb_{buffer_before}_ba_{buffer_after}.csv\"\n        val_csv_filename = f\"val_flt_ncp_tt_{turn_threshold}_bb_{buffer_before}_ba_{buffer_after}.csv\"\n    else:\n        train_csv_filename = f\"train_ncp_unfiltered.csv\"\n        val_csv_filename = f\"val_ncp_unfiltered.csv\"\n\n    #splits data into train and val\n    train_dataset_path, val_dataset_path = df_split_train_val(data_preprocessed_pd,\n                                                              train_csv_filename=train_csv_filename,\n                                                              val_csv_filename=val_csv_filename,\n                                                              save_dir=save_dir,\n                                                              train_size=train_size)\n    #gets custom train and test pytorch dataset\n    train_dataset , val_dataset = create_train_val_dataset(train_csv_file = train_dataset_path,\n                                                             val_csv_file = val_dataset_path,\n                                                             seq_len=seq_len, imgh=imgh, imgw=imgw,\n                                                             step_size=step_size, crop=crop)\n    \n    #gets dataloaders from dataset\n    train_loader, val_loader = create_train_val_loader(train_dataset, val_dataset,\n                                                       batch_size=batch_size, \n                                                       num_workers=num_workers, \n                                                       prefetch_factor=prefetch_factor,\n                                                       pin_memory=pin_memory, \n                                                       train_shuffle=train_shuffle)\n\n    return train_loader, val_loader\n\nif __name__ == '__main__':\n\n    #preprocessing csv file args\n    data_dir = '/kaggle/input/sullychen/07012018/data'\n    steering_angles_txt_path = '/kaggle/input/sullychen/07012018/data.txt'\n    save_dir = './st_lstm_norm'\n    filter = True\n    norm=True\n\n    turn_threshold = 0.08\n    buffer_before = 32 \n    buffer_after = 32\n    train_size = 0.8\n\n    #custom pytorch dataset args\n    imgh=224\n    imgw=224\n    step_size = 1\n    seq_len = 3\n    crop=True\n\n    #dataloader args\n    batch_size = 32\n    prefetch_factor = 2\n    num_workers=4\n    pin_memory=True\n    train_shuffle=False\n\n    get_loaders_for_training(\n        #preprocessing args:\n        data_dir, steering_angles_path=steering_angles_txt_path, save_dir=save_dir, filter=filter, norm=norm,\n        turn_threshold=turn_threshold, buffer_before=buffer_before, buffer_after=buffer_after, train_size=train_size,\n        #dataset args:\n        imgh=imgh, imgw=imgw, step_size=step_size, seq_len=seq_len, crop=crop, \n        #dataloader args:\n        batch_size=batch_size, prefetch_factor=prefetch_factor, num_workers=num_workers, pin_memory=pin_memory,\n        train_shuffle=train_shuffle)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T21:09:48.511774Z","iopub.execute_input":"2025-04-22T21:09:48.512442Z","iopub.status.idle":"2025-04-22T21:09:53.507996Z","shell.execute_reply.started":"2025-04-22T21:09:48.512414Z","shell.execute_reply":"2025-04-22T21:09:53.507035Z"}},"outputs":[{"name":"stdout","text":"Minimum sequence length: 77\nMaximum sequence length: 539\nTotal valid sequences: 55\nmax: 0.7016944472222223 rad: 0.012246878446989358\nmin: -0.9411666861111111 rad: -0.016426457482722874\nTrain dataset length: 8791\nVal dataset length: 2198\n./st_lstm_norm\nTotal examples: 8662 (each is 3 frames â†’ 1 target)\nTotal examples: 2159 (each is 3 frames â†’ 1 target)\nlen of train loader: 271\nlen of val loader 68\nBatch input shape: torch.Size([32, 3, 3, 224, 224])\nBatch label shape: torch.Size([32, 1])\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"def train_validate(train_loader, val_loader, optimizer, model, device, criterion, epochs=10, \n                   training_losses = None, val_losses = None, save_every=2, save_dir='/kaggle/working/checkpoints_norm_filt_sl_6_1e-5_mse'):\n\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    \n    if training_losses is None: training_losses = []\n    if val_losses is None: val_losses = []\n    \n    for epoch in range(epochs+1): \n        model.train()\n        running_train_loss = 0.0\n        correct_train = 0\n        total_train   = 0\n\n        for (batch_x, batch_y) in tqdm(train_loader, desc=f'Training {epoch+1}/{epochs}:', ncols=100):\n\n            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n\n            optimizer.zero_grad()\n            predictions = model(batch_x)\n            loss = criterion(predictions, batch_y)\n            loss.backward()\n            optimizer.step()\n\n            running_train_loss += loss.item()\n            _, pred_labels = predictions.max(dim=1)\n            correct_train += (pred_labels == batch_y).sum().item()\n            total_train += batch_y.size(0)\n\n        avg_train_loss = running_train_loss / len(train_loader)\n\n        training_losses.append(avg_train_loss)\n        print(f\"Train Loss: {avg_train_loss:.4f}\")\n\n        #validation loop\n        model.eval()\n        running_val_loss = 0.0\n        correct_val = 0\n        total_val   = 0\n        for (batch_x, batch_y) in tqdm(val_loader, desc=f'Val {epoch+1}/{epochs}:', ncols=100):\n            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n\n            with torch.no_grad():\n                predictions = model(batch_x)\n                loss = criterion(predictions, batch_y)\n\n            running_val_loss += loss.item()\n            _, pred_labels = predictions.max(dim=1)\n            correct_val += (pred_labels == batch_y).sum().item()\n            total_val += batch_y.size(0)\n\n        avg_val_loss = running_val_loss / len(val_loader)\n\n        val_losses.append(avg_val_loss)\n        print(f\"Val Loss: {avg_val_loss:.8f}\")\n\n        if epoch % save_every == 0:\n\n            checkpoint = {\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'epoch': epoch + 1,\n            'training_losses': training_losses,\n            'val_losses': val_losses,\n            }\n\n            model_path = os.path.join(save_dir, f'model_epoch{epoch+1}.pth')\n            torch.save(checkpoint, model_path)\n            print(f\"Checkpoint saved to {save_dir}\\n\")\n\n    return training_losses, val_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T23:29:23.620193Z","iopub.execute_input":"2025-04-22T23:29:23.620527Z","iopub.status.idle":"2025-04-22T23:29:23.631988Z","shell.execute_reply.started":"2025-04-22T23:29:23.620500Z","shell.execute_reply":"2025-04-22T23:29:23.631253Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"class WeightedMSE(nn.Module):\n    def __init__(self, alpha=0.1):\n        super(WeightedMSE, self).__init__()\n        self.alpha = alpha\n        \n    def forward(self, predictions, targets):\n        # squared error\n        squared_error = (predictions - targets)**2\n        \n        # weighting factor: w(y) = exp(|y|*alpha)\n        weights = torch.exp(torch.abs(targets) * self.alpha)\n        weighted_loss = squared_error * weights\n\n        return weighted_loss.mean()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T23:29:25.933590Z","iopub.execute_input":"2025-04-22T23:29:25.934541Z","iopub.status.idle":"2025-04-22T23:29:25.938862Z","shell.execute_reply.started":"2025-04-22T23:29:25.934516Z","shell.execute_reply":"2025-04-22T23:29:25.938187Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"train_dataset_path = '/kaggle/working/st_lstm_norm/train_flt_ncp_tt_0.08_bb_32_ba_32.csv'\nval_dataset_path = '/kaggle/working/st_lstm_norm/val_flt_ncp_tt_0.08_bb_32_ba_32.csv'\nseq_len = 6\nimgh = 224\nimgw = 224\nstep_size = 1\ncrop = True\n\nbatch_size = 16\nnum_workers = 4\nprefetch_factor = 2\npin_memory = True\ntrain_shuffle = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T23:29:37.299464Z","iopub.execute_input":"2025-04-22T23:29:37.300001Z","iopub.status.idle":"2025-04-22T23:29:37.304099Z","shell.execute_reply.started":"2025-04-22T23:29:37.299976Z","shell.execute_reply":"2025-04-22T23:29:37.303281Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"train_dataset, val_dataset = create_train_val_dataset(train_csv_file = train_dataset_path,\n                                                         val_csv_file = val_dataset_path,\n                                                         seq_len=seq_len, imgh=imgh, imgw=imgw,\n                                                         step_size=step_size, crop=crop)\n\n#gets dataloaders from dataset\ntrain_loader, val_loader = create_train_val_loader(train_dataset, val_dataset,\n                                                   batch_size=batch_size, \n                                                   num_workers=num_workers, \n                                                   prefetch_factor=prefetch_factor,\n                                                   pin_memory=pin_memory, \n                                                   train_shuffle=train_shuffle)\n\nfor x, y in train_loader:\n    print(x.shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T23:29:46.423819Z","iopub.execute_input":"2025-04-22T23:29:46.424550Z","iopub.status.idle":"2025-04-22T23:29:52.736168Z","shell.execute_reply.started":"2025-04-22T23:29:46.424523Z","shell.execute_reply":"2025-04-22T23:29:52.735446Z"}},"outputs":[{"name":"stdout","text":"Total examples: 8533 (each is 6 frames â†’ 1 target)\nTotal examples: 2120 (each is 6 frames â†’ 1 target)\nlen of train loader: 534\nlen of val loader 133\nBatch input shape: torch.Size([16, 6, 3, 224, 224])\nBatch label shape: torch.Size([16, 1])\ntorch.Size([16, 6, 3, 224, 224])\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"model = STConvLSTM(seq_len=6, height=224, width=224).to(device=torch.device('cuda'))\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n\ntrain_validate(train_loader, val_loader, optimizer, model, torch.device('cuda'), criterion, 10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T23:30:00.810638Z","iopub.execute_input":"2025-04-22T23:30:00.811347Z","iopub.status.idle":"2025-04-23T00:14:43.633089Z","shell.execute_reply.started":"2025-04-22T23:30:00.811319Z","shell.execute_reply":"2025-04-23T00:14:43.632406Z"}},"outputs":[{"name":"stderr","text":"Training 1/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 534/534 [03:34<00:00,  2.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0230\n","output_type":"stream"},{"name":"stderr","text":"Val 1/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:30<00:00,  4.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.02427235\nCheckpoint saved to /kaggle/working/checkpoints_norm_filt_sl_6_1e-5_mse\n\n","output_type":"stream"},{"name":"stderr","text":"Training 2/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 534/534 [03:34<00:00,  2.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0128\n","output_type":"stream"},{"name":"stderr","text":"Val 2/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:29<00:00,  4.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.02229142\n","output_type":"stream"},{"name":"stderr","text":"Training 3/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 534/534 [03:33<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0101\n","output_type":"stream"},{"name":"stderr","text":"Val 3/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:29<00:00,  4.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.02250050\nCheckpoint saved to /kaggle/working/checkpoints_norm_filt_sl_6_1e-5_mse\n\n","output_type":"stream"},{"name":"stderr","text":"Training 4/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 534/534 [03:33<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0079\n","output_type":"stream"},{"name":"stderr","text":"Val 4/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:29<00:00,  4.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.02278229\n","output_type":"stream"},{"name":"stderr","text":"Training 5/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 534/534 [03:33<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0069\n","output_type":"stream"},{"name":"stderr","text":"Val 5/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:29<00:00,  4.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.02209171\nCheckpoint saved to /kaggle/working/checkpoints_norm_filt_sl_6_1e-5_mse\n\n","output_type":"stream"},{"name":"stderr","text":"Training 6/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 534/534 [03:33<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0062\n","output_type":"stream"},{"name":"stderr","text":"Val 6/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:30<00:00,  4.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.02050576\n","output_type":"stream"},{"name":"stderr","text":"Training 7/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 534/534 [03:33<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0056\n","output_type":"stream"},{"name":"stderr","text":"Val 7/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:29<00:00,  4.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.02059266\nCheckpoint saved to /kaggle/working/checkpoints_norm_filt_sl_6_1e-5_mse\n\n","output_type":"stream"},{"name":"stderr","text":"Training 8/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 534/534 [03:33<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0053\n","output_type":"stream"},{"name":"stderr","text":"Val 8/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:30<00:00,  4.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.02127501\n","output_type":"stream"},{"name":"stderr","text":"Training 9/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 534/534 [03:33<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0048\n","output_type":"stream"},{"name":"stderr","text":"Val 9/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:30<00:00,  4.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.02066096\nCheckpoint saved to /kaggle/working/checkpoints_norm_filt_sl_6_1e-5_mse\n\n","output_type":"stream"},{"name":"stderr","text":"Training 10/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 534/534 [03:33<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0050\n","output_type":"stream"},{"name":"stderr","text":"Val 10/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:30<00:00,  4.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.02045179\n","output_type":"stream"},{"name":"stderr","text":"Training 11/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 534/534 [03:33<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0044\n","output_type":"stream"},{"name":"stderr","text":"Val 11/10:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133/133 [00:30<00:00,  4.42it/s]","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.02083567\nCheckpoint saved to /kaggle/working/checkpoints_norm_filt_sl_6_1e-5_mse\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"([0.02301717814919626,\n  0.012803915207019626,\n  0.010104719254673085,\n  0.007880288538564876,\n  0.006871680675025625,\n  0.006234068525972521,\n  0.00563676723174198,\n  0.0052824584860529065,\n  0.004815897225356078,\n  0.0050171273902201614,\n  0.004384999162903795],\n [0.02427234873897746,\n  0.022291423887700626,\n  0.022500502761178546,\n  0.022782294821699633,\n  0.0220917121557995,\n  0.02050576242399437,\n  0.020592663940646218,\n  0.021275006522182935,\n  0.020660957566828335,\n  0.02045179403194881,\n  0.020835673522958672])"},"metadata":{}}],"execution_count":52}]}